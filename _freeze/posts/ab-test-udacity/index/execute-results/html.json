{
  "hash": "516f2a7fc034d6df7e00f902ca647290",
  "result": {
    "markdown": "---\ntitle: Analyze A/B Test Results\ndescription: Udacity A/B test project\nauthor: Sean Hoyt\ndate: 3/15/2022\ncategories:\n  - python\n  - udacity\n  - analysis\n  - inference\nformat:\n  html:\n    toc: true\n---\n\n[Link to Github\nrepository](https://github.com/sdhoyt/udacity-abtest-project)\n\n# Introduction\n\nA/B tests are very commonly performed by data analysts and data\nscientists. For this project, you will be working to understand the\nresults of an A/B test run by an e-commerce website. Your goal is to\nwork through this notebook to help the company understand if they\nshould: - Implement the new webpage, - Keep the old webpage, or -\nPerhaps run the experiment longer to make their decision.\n\n# Part I - Probability\n\nTo get started, let's import our libraries.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\nrandom.seed(42)\n```\n:::\n\n\n### Import the Data\n\nNow, read in the `ab_data.csv` data. Store it in `df`. Below is the\ndescription of the data, there are a total of 5 columns:\n\n<center>\n\n| Data columns | Purpose                                                                                                                                                                                                                                                                                                                                                   |               Valid values |\n|--------------------------|:---------------------------|-----------------:|\n| user_id      | Unique ID                                                                                                                                                                                                                                                                                                                                                 |               Int64 values |\n| timestamp    | Time stamp when the user visited the webpage                                                                                                                                                                                                                                                                                                              |                         \\- |\n| group        | In the current A/B experiment, the users are categorized into two broad groups. <br>The `control` group users are expected to be served with `old_page`; and `treatment` group users are matched with the `new_page`. <br>However, **some inaccurate rows** are present in the initial data, such as a `control` group user is matched with a `new_page`. | `['control', 'treatment']` |\n| landing_page | It denotes whether the user visited the old or new webpage.                                                                                                                                                                                                                                                                                               | `['old_page', 'new_page']` |\n| converted    | It denotes whether the user decided to pay for the company's product. Here, `1` means yes, the user bought the product.                                                                                                                                                                                                                                   |                   `[0, 1]` |\n\n</center>\n\nUse your dataframe to answer the questions in Quiz 1 of the classroom.\n\n**a.** Read in the dataset from the `ab_data.csv` file and take a look\nat the top few rows here:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('data/ab_data.csv')\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>timestamp</th>\n      <th>group</th>\n      <th>landing_page</th>\n      <th>converted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>851104</td>\n      <td>2017-01-21 22:11:48.556739</td>\n      <td>control</td>\n      <td>old_page</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>804228</td>\n      <td>2017-01-12 08:01:45.159739</td>\n      <td>control</td>\n      <td>old_page</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>661590</td>\n      <td>2017-01-11 16:55:06.154213</td>\n      <td>treatment</td>\n      <td>new_page</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>853541</td>\n      <td>2017-01-08 18:28:03.143765</td>\n      <td>treatment</td>\n      <td>new_page</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>864975</td>\n      <td>2017-01-21 01:52:26.210827</td>\n      <td>control</td>\n      <td>old_page</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**b.** Use the cell below to find the number of rows in the dataset.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(294478, 5)\n```\n:::\n:::\n\n\n**c.** The number of unique users in the dataset.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf.user_id.nunique()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n290584\n```\n:::\n:::\n\n\n**d.** The proportion of users converted.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndf.converted.mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.11965919355605512\n```\n:::\n:::\n\n\n**e.** The number of times when the \"group\" is `treatment` but\n\"landing_page\" is not a `new_page`.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nwrong_treatment = ((df['group'] == 'treatment') & (df['landing_page'] != 'new_page')).sum()\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nwrong_control = ((df['group'] == 'control') & (df['landing_page'] != 'old_page')).sum()\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nwrong_treatment + wrong_control\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n3893\n```\n:::\n:::\n\n\n**f.** Do any of the rows have missing values?\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 294478 entries, 0 to 294477\nData columns (total 5 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   user_id       294478 non-null  int64 \n 1   timestamp     294478 non-null  object\n 2   group         294478 non-null  object\n 3   landing_page  294478 non-null  object\n 4   converted     294478 non-null  int64 \ndtypes: int64(2), object(3)\nmemory usage: 11.2+ MB\n```\n:::\n:::\n\n\nNo missing values.\n\n### Clean the Data\n\nIn a particular row, the **group** and **landing_page** columns should\nhave either of the following acceptable values:\n\n| user_id | timestamp | group       | landing_page | converted |\n|---------|-----------|-------------|--------------|-----------|\n| XXXX    | XXXX      | `control`   | `old_page`   | X         |\n| XXXX    | XXXX      | `treatment` | `new_page`   | X         |\n\nIt means, the `control` group users should match with `old_page`; and\n`treatment` group users should matched with the `new_page`.\n\nHowever, for the rows where `treatment` does not match with `new_page`\nor `control` does not match with `old_page`, we cannot be sure if such\nrows truly received the new or old wepage.\n\nUse **Quiz 2** in the classroom to figure out how should we handle the\nrows where the group and landing_page columns don't match?\n\n**a.** Now use the answer to the quiz to create a new dataset that meets\nthe specifications from the quiz. Store your new dataframe in **df2**.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Remove the inaccurate rows, and store the result in a new dataframe df2\ndf2 = df[((df['group'] == 'treatment') & (df['landing_page'] == 'new_page')) |\n          ((df['group'] == 'control') & (df['landing_page'] == 'old_page'))]\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Double Check all of the incorrect rows were removed from df2 - \n# Output of the statement below should be 0\nwrong_treatment2 = ((df2['group'] == 'treatment') & (df2['landing_page'] != 'new_page')).sum()\nwrong_control2 = ((df2['group'] == 'control') & (df2['landing_page'] != 'old_page')).sum()\nwrong_treatment2 + wrong_control2\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n0\n```\n:::\n:::\n\n\nUse **df2** and the cells below to answer questions for **Quiz 3** in\nthe classroom.\n\n**a.** How many unique **user_id**s are in **df2**?\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndf2.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n(290585, 5)\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndf2.user_id.nunique()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n290584\n```\n:::\n:::\n\n\n**b.** There is one **user_id** repeated in **df2**. What is it?\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndf2.user_id.value_counts().head()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n773192    2\n851104    1\n688307    1\n718297    1\n838144    1\nName: user_id, dtype: int64\n```\n:::\n:::\n\n\n**c.** Display the rows for the duplicate **user_id**?\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndf2[df2['user_id'] == 773192]\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>timestamp</th>\n      <th>group</th>\n      <th>landing_page</th>\n      <th>converted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1899</th>\n      <td>773192</td>\n      <td>2017-01-09 05:37:58.781806</td>\n      <td>treatment</td>\n      <td>new_page</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2893</th>\n      <td>773192</td>\n      <td>2017-01-14 02:55:59.590927</td>\n      <td>treatment</td>\n      <td>new_page</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**d.** Remove **one** of the rows with a duplicate **user_id**, from the\n**df2** dataframe.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Remove one of the rows with a duplicate user_id..\n# Hint: The dataframe.drop_duplicates() may not work in this case because the rows with duplicate user_id are not entirely identical.\n# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html\ndf2.drop_duplicates(subset='user_id', inplace=True)\n# Check again if the row with a duplicate user_id is deleted or not\ndf2.user_id.value_counts().head()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_89115/377076188.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df2.drop_duplicates(subset='user_id', inplace=True)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n851104    1\n688307    1\n718297    1\n838144    1\n728209    1\nName: user_id, dtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndf2.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n(290584, 5)\n```\n:::\n:::\n\n\n### Calculate Click Through Rates\n\nUse **df2** in the cells below to answer the quiz questions related to\n**Quiz 4** in the classroom.\n\n**a.** What is the probability of an individual converting regardless of\nthe page they receive?<br><br>\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ndf2.converted.mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n0.11959708724499628\n```\n:::\n:::\n\n\n**b.** Given that an individual was in the `control` group, what is the\nprobability they converted?\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ncontrol_cr = df2[df['group'] == 'control'].converted.mean()\ncontrol_cr\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_89115/2805132061.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  control_cr = df2[df['group'] == 'control'].converted.mean()\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n0.1203863045004612\n```\n:::\n:::\n\n\n**c.** Given that an individual was in the `treatment` group, what is\nthe probability they converted?\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ntreatment_cr = df2[df['group'] == 'treatment'].converted.mean()\ntreatment_cr\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_89115/1278796718.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  treatment_cr = df2[df['group'] == 'treatment'].converted.mean()\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n0.11880806551510564\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# Calculate the actual difference (obs_diff) between the conversion rates for the two groups.\nobs_diff = treatment_cr - control_cr\nobs_diff\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n-0.0015782389853555567\n```\n:::\n:::\n\n\n**d.** What is the probability that an individual received the new page?\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n(df2['landing_page'] == 'new_page').mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n0.5000619442226688\n```\n:::\n:::\n\n\n**e.** Consider your results from parts (a) through (d) above, and\nexplain below whether the new `treatment` group users lead to more\nconversions.\n\n<font color='red'>Based on the conversion rates for the treatment and\nthe control groups, there seemed to be a lower conversion rate for the\ntreatment group than the control group, indicating that the new page may\nnot lead to more purchases than the old page.</font>\n\n# Part II - A/B Test\n\nSince a timestamp is associated with each event, you could run a\nhypothesis test continuously as long as you observe the events.\n\nHowever, then the hard questions would be: - Do you stop as soon as one\npage is considered significantly better than another or does it need to\nhappen consistently for a certain amount of time?\\\n- How long do you run to render a decision that neither page is better\nthan another?\n\nThese questions are the difficult parts associated with A/B tests in\ngeneral.\n\n### Hypothesis Test Setup\n\nFor now, consider you need to make the decision just based on all the\ndata provided.\n\n> Recall that you just calculated that the \"converted\" probability (or\n> rate) for the old page is *slightly* higher than that of the new page\n> (ToDo 1.4.c).\n\nIf you want to assume that the old page is better unless the new page\nproves to be definitely better at a Type I error rate of 5%, what should\nbe your null and alternative hypotheses ($H_0$ and $H_1$)?\n\nYou can state your hypothesis in terms of words or in terms of $p_{old}$\nand $p_{new}$, which are the \"converted\" probability (or rate) for the\nold and new pages respectively.\n\n<font color=\"red\">The Null Hypothesis is that the new page leads to as many or less\nconversions than the old page. The Alternative Hypothesis is that the\nnew page leads to more conversions than the old page.</font>\n\n$$H_0: p_{new} - p_{old} \\leq 0$$ $$H_1: p_{new} - p_{old} > 0$$\n\n### Null Hypothesis $H_0$ Testing\n\nUnder the null hypothesis $H_0$, assume that $p_{new}$ and $p_{old}$ are\nequal. Furthermore, assume that $p_{new}$ and $p_{old}$ both are equal\nto the **converted** success rate in the `df2` data regardless of the\npage. So, our assumption is: <br><br>\n\n<center>\n\n$p_{new}$ = $p_{old}$ = $p_{population}$\n\n</center>\n\nIn this section, you will:\n\n-   Simulate (bootstrap) sample data set for both groups, and compute\n    the \"converted\" probability $p$ for those samples.\n\n-   Use a sample size for each group equal to the ones in the `df2`\n    data.\n\n-   Compute the difference in the \"converted\" probability for the two\n    samples above.\n\n-   Perform the sampling distribution for the \"difference in the\n    converted probability\" between the two simulated-samples over 10,000\n    iterations; and calculate an estimate.\n\nUse the cells below to provide the necessary parts of this simulation.\nYou can use **Quiz 5** in the classroom to make sure you are on the\nright track.\n\n**a.** What is the **conversion rate** for $p_{new}$ under the null\nhypothesis?\n\n<font color='red'>Because our Null hypothesis assumes that\n$p_{new} = p_{old}$, the conversion rate for the new and old pages\nshould be equal to the entire conversion rate in the data, therefore\nbeing the same.</font>\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nconv_rate_new = df2.converted.mean()\nconv_rate_new\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n0.11959708724499628\n```\n:::\n:::\n\n\n**b.** What is the **conversion rate** for $p_{old}$ under the null\nhypothesis?\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nconv_rate_old = df2.converted.mean()\nconv_rate_old\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n0.11959708724499628\n```\n:::\n:::\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nconv_rate_new - conv_rate_old\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n0.0\n```\n:::\n:::\n\n\n**c.** What is $n_{new}$, the number of individuals in the treatment\ngroup? <br><br> *Hint*: The treatment group users are shown the new\npage.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ntreatment_rows = df2[df2['group'] == 'treatment'].shape[0]\ntreatment_rows\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n145310\n```\n:::\n:::\n\n\n**d.** What is $n_{old}$, the number of individuals in the control\ngroup?\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ncontrol_rows = df2[df2['group'] == 'control'].shape[0]\ncontrol_rows\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n145274\n```\n:::\n:::\n\n\n**e. Simulate Sample for the `treatment` Group**<br> Simulate $n_{new}$\ntransactions with a conversion rate of $p_{new}$ under the null\nhypothesis. <br><br>\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n# Simulate a Sample for the treatment Group\n# 0 or 1 outcome for the number of people who got the treatment\ntreatment_sample = np.random.choice([0, 1], size=treatment_rows, p=[1-conv_rate_new, conv_rate_new])\n```\n:::\n\n\n**f. Simulate Sample for the `control` Group** <br> Simulate $n_{old}$\ntransactions with a conversion rate of $p_{old}$ under the null\nhypothesis. <br> Store these $n_{old}$ 1's and 0's in the\n`old_page_converted` numpy array.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n# Simulate a Sample for the control Group\ncontrol_sample = np.random.choice([0, 1], size=control_rows, p=[1-conv_rate_old, conv_rate_old])\n```\n:::\n\n\n**g.** Find the difference in the \"converted\" probability\n$(p{'}_{new}$ - $p{'}_{old})$ for your simulated samples from the parts\n(e) and (f) above.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\ntreatment_sample.mean() - control_sample.mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n-0.000635345208159388\n```\n:::\n:::\n\n\n**h. Sampling distribution** <br> Re-create `new_page_converted` and\n`old_page_converted` and find the $(p{'}_{new}$ - $p{'}_{old})$ value\n10,000 times using the same simulation process you used in parts (a)\nthrough (g) above.\n\n<br> Store all $(p{'}_{new}$ - $p{'}_{old})$ values in a NumPy array\ncalled `p_diffs`.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n# sampling distribution\n# use the binomal function (because we are sampling 1s or 0s) to take advantage of \n# the function speed over looping/sampling\n\n# returns the number number of treatment rows that converted (based on the conversion rate and binomical dist) for\n# 10,000 samples and divides by the number of treatment rows to get the pct conversion for the samples\ntreatment_sample = np.random.binomial(treatment_rows, conv_rate_new, 10000) / treatment_rows\ncontrol_sample = np.random.binomial(control_rows, conv_rate_old, 10000) / control_rows\np_diffs = treatment_sample - control_sample\n```\n:::\n\n\n**i. Histogram**<br> Plot a histogram of the **p_diffs**. Does this plot\nlook like what you expected? Use the matching problem in the classroom\nto assure you fully understand what was computed here.<br><br>\n\nAlso, use `plt.axvline()` method to mark the actual difference observed\nin the `df2` data (recall `obs_diff`), in the chart.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nplt.hist(p_diffs);\nplt.xlabel(\"conversion rate probability difference\")\nplt.ylabel(\"Count\")\nplt.title(\"Conversion Rates Sample from the Null\")\nplt.axvline(obs_diff, color='red')\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n<matplotlib.lines.Line2D at 0x7fecb45db5b0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-33-output-2.png){}\n:::\n:::\n\n\n**j.** What proportion of the **p_diffs** are greater than the actual\ndifference observed in the `df2` data?\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\n(p_diffs > obs_diff).mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\n0.9098\n```\n:::\n:::\n\n\n**k.** Please explain in words what you have just computed in part **j**\nabove.\\\n- What is this value called in scientific studies?\\\n- What does this value signify in terms of whether or not there is a\ndifference between the new and old pages? *Hint*: Compare the value\nabove with the \"Type I error rate (0.05)\".\n\n<font color='red'>The p-value calculated above of 0.902, considering a\nType I error rate of (0.05) clearly shows that we cannot reject the null\nhypothesis that the old page has a conversion equal to or greater than\nthe new page. By bootstrap sampling from a Null that the two pages had\nequal conversion rates, we can see from the historgram above and the\np-value that the observed conversion rate is much less than large\nmajority of the null distribution. Because the p-value and its extreme\ntoward the alternative hypothesis (greater than the null), contain most\nof the null distribution, we do not have evidence to reject the null\nhypothesis.</font>\n\n**l. Using Built-in Methods for Hypothesis Testing**<br> We could also\nuse a built-in to achieve similar results. Though using the built-in\nmight be easier to code, the above portions are a walkthrough of the\nideas that are critical to correctly thinking about statistical\nsignificance.\n\nFill in the statements below to calculate the: - `convert_old`: number\nof conversions with the old_page - `convert_new`: number of conversions\nwith the new_page - `n_old`: number of individuals who were shown the\nold_page - `n_new`: number of individuals who were shown the new_page\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nimport statsmodels.api as sm\n\n# number of conversions with the old_page\nconvert_old = df2[df2['landing_page'] == \"old_page\"].converted.sum()\n\n# number of conversions with the new_page\nconvert_new = df2[df2['landing_page'] == \"new_page\"].converted.sum()\n\n# number of individuals who were shown the old_page\nn_old = df2[df2['landing_page'] == \"old_page\"].shape[0]\n\n# number of individuals who received new_page\nn_new = df2[df2['landing_page'] == \"new_page\"].shape[0]\n```\n:::\n\n\n**m.** Now use `sm.stats.proportions_ztest()` to compute your test\nstatistic and p-value.\n[Here](https://www.statsmodels.org/stable/generated/statsmodels.stats.proportion.proportions_ztest.html)\nis a helpful link on using the built in.\n\nThe syntax is:\n\n```python\nproportions_ztest(count_array, nobs_array, alternative='larger')\n```\n\nwhere, - `count_array` = represents the number of \"converted\" for each\ngroup - `nobs_array` = represents the total number of observations\n(rows) in each group - `alternative` = choose one of the values from\n`[‘two-sided’, ‘smaller’, ‘larger’]` depending upon two-tailed,\nleft-tailed, or right-tailed respectively.\n\nThe built-in function above will return the z_score, p_value.\n\n### About the two-sample z-test\nRecall that you have plotted a distribution `p_diffs` representing the\ndifference in the \"converted\" probability  $(p{'}_{new}-p{'}_{old})$  for your two simulated samples 10,000 times. \n\nAnother way for comparing the mean of two independent and normal distribution is a **two-sample z-test**. You can perform the Z-test to calculate the Z_score, as shown in the equation below:\n\n$$Z_{score} = \\frac{ (p{'}_{new}-p{'}_{old}) - (p_{new}  -  p_{old})}{ \\sqrt{ \\frac{\\sigma^{2}_{new} }{n_{new}} + \\frac{\\sigma^{2}_{old} }{n_{old}}  } }$$\n\nwhere,\n- $p{'}$ is the \"converted\" success rate in the sample\n- $p_{new}$ and $p_{old}$ are the \"converted\" success rate for the two groups in the population. \n- $\\sigma_{new}$ and $\\sigma_{new}$ are the standard deviation for the two groups in the population. \n- $n_{new}$ and $n_{old}$ represent the size of the two groups or samples (it's same in our case)\n\n\n>Z-test is performed when the sample size is large, and the population variance is known. The z-score represents the distance between the two \"converted\" success rates in terms of the standard error. \n\nNext step is to make a decision to reject or fail to reject the null hypothesis based on comparing these two values: \n- $Z_{score}$\n- $Z_{\\alpha}$ or $Z_{0.05}$, also known as critical value at 95% confidence interval.  $Z_{0.05}$ is 1.645 for one-tailed tests,  and 1.960 for two-tailed test. You can determine the $Z_{\\alpha}$ from the z-table manually. \n\nDecide if your hypothesis is either a two-tailed, left-tailed, or right-tailed test. Accordingly, reject OR fail to reject the  null based on the comparison between $Z_{score}$ and $Z_{\\alpha}$. We determine whether or not the $Z_{score}$ lies in the \"rejection region\" in the distribution. In other words, a \"rejection region\" is an interval where the null hypothesis is rejected iff the $Z_{score}$ lies in that region.\n\nReference: \n- Example 9.1.2 on this [page](https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Book%3A_Introductory_Statistics_(Shafer_and_Zhang)/09%3A_Two-Sample_Problems/9.01%3A_Comparison_of_Two_Population_Means-_Large_Independent_Samples), courtesy www.stats.libretexts.org\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nimport statsmodels.api as sm\n# ToDo: Complete the sm.stats.proportions_ztest() method arguments\nz_score, p_value = sm.stats.proportions_ztest(\n    [convert_new, convert_old],\n    [n_new, n_old],\n    alternative='larger')\nprint(z_score, p_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-1.3109241984234394 0.9050583127590245\n```\n:::\n:::\n\n\n**n.** What do the z-score and p-value you computed in the previous\nquestion mean for the conversion rates of the old and new pages? Do they\nagree with the findings in parts **j.** and **k.**?<br><br>\n\n<font color='red'>The p-value is very close to (and effectively the same\nas) the p-value calculated previously. Therefore, this p-value does\nindicate the same result that the null hypothesis cannot be\nrejected. </font>\n\n# Part III - A regression approach\n\n### Interential Logistic Regression\n\nIn this final part, you will see that the result you achieved in the A/B\ntest in Part II above can also be achieved by performing\nregression.<br><br>\n\n**a.** Since each row in the `df2` data is either a conversion or no\nconversion, what type of regression should you be performing in this\ncase?\n\nBecause it is binary (2 choices), logistic regression\nshould be used.\n\n**b.** The goal is to use **statsmodels** library to fit the regression\nmodel you specified in part **a.** above to see if there is a\nsignificant difference in conversion based on the page-type a customer\nreceives. However, you first need to create the following two columns in\nthe `df2` dataframe: 1. `intercept` - It should be `1` in the entire\ncolumn. 2. `ab_page` - It's a dummy variable column, having a value `1`\nwhen an individual receives the **treatment**, otherwise `0`.\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\ndf2['intercept'] = 1\ndf2[['ab_page', 'old_page']]=pd.get_dummies(df['landing_page'])\ndf2.drop('old_page', axis=1, inplace=True)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_89115/2238491058.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df2['intercept'] = 1\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_89115/2238491058.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df2[['ab_page', 'old_page']]=pd.get_dummies(df['landing_page'])\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_89115/2238491058.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df2[['ab_page', 'old_page']]=pd.get_dummies(df['landing_page'])\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_89115/2238491058.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df2.drop('old_page', axis=1, inplace=True)\n```\n:::\n:::\n\n\n**c.** Use **statsmodels** to instantiate your regression model on the\ntwo columns you created in part (b). above, then fit the model to\npredict whether or not an individual converts.\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nlogit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\nresult = logit_mod.fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.366118\n         Iterations 6\n```\n:::\n:::\n\n\n**d.** Provide the summary of your model below, and use it as necessary\nto answer the following questions.\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nresult.summary2()\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```{=html}\n<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212780.3502</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-05-22 00:11</td>       <td>BIC:</td>        <td>212801.5095</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>1</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n</tr>\n<tr>\n     <td>Df Residuals:</td>         <td>290582</td>        <td>LLR p-value:</td>      <td>0.18988</td>  \n</tr>\n<tr>\n      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n</tr>\n<tr>\n    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n</tr>\n<tr>\n  <th>intercept</th> <td>-1.9888</td>  <td>0.0081</td>  <td>-246.6690</td> <td>0.0000</td> <td>-2.0046</td> <td>-1.9730</td>\n</tr>\n<tr>\n  <th>ab_page</th>   <td>-0.0150</td>  <td>0.0114</td>   <td>-1.3109</td>  <td>0.1899</td> <td>-0.0374</td> <td>0.0074</td> \n</tr>\n</table>\n```\n:::\n:::\n\n\n**e.** What is the p-value associated with **ab_page**? Why does it\ndiffer from the value you found in **Part II**?<br><br>\n\n<font color='red'>The null hypothsis in the regression model is that the\ncoefficient has a value of 0. This says that there is no relationship\nbetween the pages and converting. This is slightly different from the\nnull hypothesis from the A/B test because it implies a two-sided test as\nopposed to a one-sided test in the hypothesis test from Part II.</font>\n\n**f.** Now, you are considering other things that might influence\nwhether or not an individual converts. Discuss why it is a good idea to\nconsider other factors to add into your regression model. Are there any\ndisadvantages to adding additional terms into your regression model?\n\n<font color='red'>Other factors you could look at are how the time of\nday or location affect whether a user converts. The other question to\nask is whether the test ran for a long enough time so that returning\ncustomers could adjust to the new page layout. One disadvantage that\ncould result from adding more explantory variables to a linear model is\nmulticolinearity. If two of the explanatory variables are also\ncorrelated, this can cause strange values in the coefficients to show\nup, leading to incorrect interpretations.</font>\n\n**g. Adding countries**<br> Now along with testing if the conversion\nrate changes for different pages, also add an effect based on which\ncountry a user lives in.\n\n1.  You will need to read in the **countries.csv** dataset and merge\n    together your `df2` datasets on the appropriate rows. You call the\n    resulting dataframe `df_merged`.\n    [Here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html)\n    are the docs for joining tables.\n\n2.  Does it appear that country had an impact on conversion? To answer\n    this question, consider the three unique values,\n    `['UK', 'US', 'CA']`, in the `country` column. Create dummy\n    variables for these country columns.\n\nProvide the statistical output as well as a written response to answer\nthis question.\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\n# Read the countries.csv\ncountries = pd.read_csv('data/countries.csv')\n```\n:::\n\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\n# Join with the df2 dataframe\ndf2= df2.set_index('user_id').join(countries.set_index('user_id'), how='inner', on='user_id', lsuffix='df2', rsuffix='countries')\n```\n:::\n\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\n# Create the necessary dummy variables\ndf2['intercept'] = 1\ndf2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country'])\n```\n:::\n\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\n# Fit your model, and summarize the results\nlogit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'UK', 'CA']])\nresult = logit_mod.fit()\nresult.summary2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.366113\n         Iterations 6\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=42}\n```{=html}\n<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212781.1253</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-05-22 00:11</td>       <td>BIC:</td>        <td>212823.4439</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>3</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n</tr>\n<tr>\n     <td>Df Residuals:</td>         <td>290580</td>        <td>LLR p-value:</td>      <td>0.17599</td>  \n</tr>\n<tr>\n      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n</tr>\n<tr>\n    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n</tr>\n<tr>\n  <th>intercept</th> <td>-1.9893</td>  <td>0.0089</td>  <td>-223.7628</td> <td>0.0000</td> <td>-2.0067</td> <td>-1.9718</td>\n</tr>\n<tr>\n  <th>ab_page</th>   <td>-0.0149</td>  <td>0.0114</td>   <td>-1.3069</td>  <td>0.1912</td> <td>-0.0374</td> <td>0.0075</td> \n</tr>\n<tr>\n  <th>UK</th>        <td>0.0099</td>   <td>0.0133</td>   <td>0.7433</td>   <td>0.4573</td> <td>-0.0162</td> <td>0.0359</td> \n</tr>\n<tr>\n  <th>CA</th>        <td>-0.0408</td>  <td>0.0269</td>   <td>-1.5161</td>  <td>0.1295</td> <td>-0.0934</td> <td>0.0119</td> \n</tr>\n</table>\n```\n:::\n:::\n\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\n1/np.exp(-.0149)\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\n1.0150115583846535\n```\n:::\n:::\n\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\nnp.exp(.0099)\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\n1.0099491671175422\n```\n:::\n:::\n\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\n1/np.exp(-0.408)\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\n1.5038071611701118\n```\n:::\n:::\n\n\n<font color='red'>Based on the logistic regression results above, we\ncannot reject the null hypothesis for any of the explanatory variables.\nBased on the results, the old page more 1.015 times effective at\nconverting users, users in Canada are 1.01 times less likely to convert\nto the convert compared to US customers and UK customers are 1.5 times\nmore likely to convert compared to US customers. However, the p-values\nare too large to convince us that there is a statistically significat\ncorrelation in effect here. The differences are small and since we did\nnot have evidence to reject the null hypothesis, it would be best to\nstay with the old page.</font>\n\n**h. Fit your model and obtain the results**<br> Though you have now\nlooked at the individual factors of country and page on conversion, we\nwould now like to look at an interaction between page and country to see\nif are there significant effects on conversion. **Create the necessary\nadditional columns, and fit the new model.**\n\nProvide the summary results (statistical output), and your conclusions\n(written response) based on the results.\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\ndf2['UK_abpage'] = df2['UK'] * df2['ab_page']\ndf2['CA_abpage'] = df2['CA'] * df2['ab_page']\n```\n:::\n\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\n# Fit your model, and summarize the results\nlogit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'UK', 'CA', 'UK_abpage', 'CA_abpage']])\nresult = logit_mod.fit()\nresult.summary2()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.366109\n         Iterations 6\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=47}\n```{=html}\n<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212782.6602</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-05-22 00:11</td>       <td>BIC:</td>        <td>212846.1381</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>5</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n</tr>\n<tr>\n     <td>Df Residuals:</td>         <td>290578</td>        <td>LLR p-value:</td>      <td>0.19199</td>  \n</tr>\n<tr>\n      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n</tr>\n<tr>\n    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n</tr>\n<tr>\n  <th>intercept</th> <td>-1.9865</td>  <td>0.0096</td>  <td>-206.3440</td> <td>0.0000</td> <td>-2.0053</td> <td>-1.9676</td>\n</tr>\n<tr>\n  <th>ab_page</th>   <td>-0.0206</td>  <td>0.0137</td>   <td>-1.5052</td>  <td>0.1323</td> <td>-0.0473</td> <td>0.0062</td> \n</tr>\n<tr>\n  <th>UK</th>        <td>-0.0057</td>  <td>0.0188</td>   <td>-0.3057</td>  <td>0.7598</td> <td>-0.0426</td> <td>0.0311</td> \n</tr>\n<tr>\n  <th>CA</th>        <td>-0.0175</td>  <td>0.0377</td>   <td>-0.4652</td>  <td>0.6418</td> <td>-0.0914</td> <td>0.0563</td> \n</tr>\n<tr>\n  <th>UK_abpage</th> <td>0.0314</td>   <td>0.0266</td>   <td>1.1807</td>   <td>0.2377</td> <td>-0.0207</td> <td>0.0835</td> \n</tr>\n<tr>\n  <th>CA_abpage</th> <td>-0.0469</td>  <td>0.0538</td>   <td>-0.8718</td>  <td>0.3833</td> <td>-0.1523</td> <td>0.0585</td> \n</tr>\n</table>\n```\n:::\n:::\n\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\n1/np.exp(-.0206)\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\n1.020813644503746\n```\n:::\n:::\n\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\n1/np.exp(-.0057)\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\n1.0057162759095335\n```\n:::\n:::\n\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\n1/np.exp(-0.175)\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```\n1.191246216612358\n```\n:::\n:::\n\n\n<font color='red'>Using the interactions, the old page shows a 1.02\ntimes higher conversion than the new page. However, both the UK and CA\nnow show less conversion than the US (UK 1.005 times less, CA 1.19 times\nless). For the UK/Page interaction, there is a positive correlation with\nconversion and a negative correlation for the CA/Page interaction,\nhowever, because these are higher order terms, they are more difficult\nto use for inference compared to the others where we can clearly see the\neffect it is having on the conversion. However as with the case above,\nthe p-values are too large to be convinced of any statistical\nsignificance in the correlation.</font>\n\n# Conclusion\n\n<font color=\"red\">In this analysis, multiple techniques were utilized to\ndetermine whether a new landing page should be used in lieu of the old\npage. The results of the hypothesis test showed a p-value of 0.9,\nindicating that we cannot reject the null hypothesis that the old page\nleads to just as many, if not more conversions than the new page. Using\nthe data to build a logistic regression model also showed high p-values,\nhinting that the correlations found in the model are not statistically\nsignificant. The recommendation resulting from this analysis to to\ncontinue use of the old page rather than converting to the new\npage.</font>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}