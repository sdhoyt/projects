[
  {
    "objectID": "posts/data-viz-udacity/index.html",
    "href": "posts/data-viz-udacity/index.html",
    "title": "Bike Share Station Supply",
    "section": "",
    "text": "Data and Code are available here"
  },
  {
    "objectID": "posts/data-viz-udacity/index.html#investigation-overview",
    "href": "posts/data-viz-udacity/index.html#investigation-overview",
    "title": "Bike Share Station Supply",
    "section": "Investigation Overview",
    "text": "Investigation Overview\nBike rental demand tends to vary along three attributes: station, gender, and day of the week. These three attributes are important factors in ensuring that the supply of bike rentals meets customer demand. Each individual station must contain an adequate supply of bikes to meet the demand of users at that station on any particular day. Gender can also be generalized to anticipate what size bikes should be stocked at a particular station to meet general differences in bike sizing for different genders. These factors can provide key insights into ensuring that the bike sharing stations provide adequate supply for their customers."
  },
  {
    "objectID": "posts/data-viz-udacity/index.html#dataset-overview",
    "href": "posts/data-viz-udacity/index.html#dataset-overview",
    "title": "Bike Share Station Supply",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nThe bike sharing dataset includes information about individual rides made in a bike-sharing system covering the greater San Francisco Bay area. The datas used in this analysis is limited to February 1, 2019 to March 1, 2019.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# suppress warnings from final output\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\n# get the dat of week from a date time\ndef get_day_of_week(df, col):\n    df['day_of_week'] = df[col].dt.day_of_week\n    # make a list of days of week to replace the numbers (0-6) generated from the dt.day_of_week\n    days_of_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n    # replace the numbered values with the strings of the days of the week\n    df['day_of_week'] = df.day_of_week.replace(to_replace=[0, 1, 2, 3, 4, 5, 6], value=days_of_week)\n    # make the days categorical types\n    days = pd.api.types.CategoricalDtype(ordered=True, categories=days_of_week)\n    # cast the days of week column as the days categorical type\n    return df['day_of_week'].astype(days)\n\n\n# Perform all the data wrangling and feature engineering needed for the visualizations\n\n# load the data and parse date columns as datetimes\nbike_raw = pd.read_csv(\"data/201902-fordgobike-tripdata.csv\", parse_dates=[\"start_time\", \"end_time\"])\n# drop the features that won't be explored\nbike = bike_raw.drop(labels=['start_station_id', 'end_station_id'], axis=1)\n# get the day of the week from the datetime\nbike[\"day_of_week\"] = get_day_of_week(bike, \"start_time\")"
  },
  {
    "objectID": "posts/data-viz-udacity/index.html#rental-trends-throughout-the-week",
    "href": "posts/data-viz-udacity/index.html#rental-trends-throughout-the-week",
    "title": "Bike Share Station Supply",
    "section": "Rental Trends Throughout the Week",
    "text": "Rental Trends Throughout the Week\nBike rental frequencies are relatively consistent throughout the weekdays, but decrease during the weekend. This suggests that weekday traffic to the stations should be used to determine the supply of bikes since the weekdays are when rentals peak. However, we need more information to determine the proper sizing of bikes needed (based on gender size generalizations) and what supply is needed for each station.\n\ng = sns.countplot(data=bike, y=\"day_of_week\", color=sns.color_palette()[0]);\n\ng.set_title(\"Overall Rentals By Day of Week\");\ng.set(xlabel=\"Number of rentals\", ylabel=\"Day of the week\");\ng.annotate(\"Drop in users during weekend\", xy=(16000, 5.5), xytext = (20000, 5.5), arrowprops = {\"arrowstyle\":\"->\", \"color\":\"gray\"});"
  },
  {
    "objectID": "posts/data-viz-udacity/index.html#user-genders",
    "href": "posts/data-viz-udacity/index.html#user-genders",
    "title": "Bike Share Station Supply",
    "section": "User Genders",
    "text": "User Genders\nA majority of the users in the month of data were male. There is still a significant portion of female riders and a small portion of riders who identified as other. This information, if it were broken down by station, can be useful in determining the sizing of bikes that are needed to meet customer demand.\n\ng = sns.countplot(data=bike, x=\"member_gender\", order = bike['member_gender'].value_counts().index);\ng.set_title(\"Rentals by Gender\");\ng.set(ylabel=\"Number of rentals\", xlabel=\"Gender\");"
  },
  {
    "objectID": "posts/data-viz-udacity/index.html#station-demand",
    "href": "posts/data-viz-udacity/index.html#station-demand",
    "title": "Bike Share Station Supply",
    "section": "Station Demand",
    "text": "Station Demand\nBy taking our data on bike rentals per week and gender breakdown of users and combining it with information on the stations, we can visualize the mean number of bikes that are needed at each station for each day of the week and the bike sizes that are needed (using generalizations on gender size). For example, we can see from the visualization below that on Mondays at the Berry St at 4th station, there should be stock to accomadate an average of around 80 rentals for bikes that should be generally sized for males. We can also see the standard error which can give a range, in this case around 50 to 115 rentals, which provides information on how we can expect the number of rentals to vary for that day. This overview of the data can be vital for the company to ensure that the stations are stocked daily to meet the demand of their expected customers.\n\n# Perform feature engineering needed for visualization\n\n# get the date for each rental\nbike[\"date\"] = bike.start_time.dt.floor(\"D\")\n# count the number of rentals for each date, station, and gender\nbike_rentals = bike.groupby([\"start_station_name\", \"member_gender\", \"date\"]).agg(num_rentals = ('start_time', 'count')).reset_index()\n# get day of week for each date\nbike_rentals[\"day_of_week\"] = get_day_of_week(bike_rentals, \"date\")\n# get the six stations with the most total rentals\ntop_six_stations = bike_rentals.groupby(\"start_station_name\").num_rentals.sum().reset_index().sort_values(\"num_rentals\", ascending=False).head(6).start_station_name.values \n# filter the bike rentals for the top six stations\ntop_six_station_rentals = bike_rentals[bike_rentals[\"start_station_name\"].isin(top_six_stations)]\n\n\ng = sns.catplot(\n    data=top_six_station_rentals, \n    x=\"day_of_week\", y=\"num_rentals\", \n    hue=\"member_gender\", kind=\"point\",\n    hue_order=[\"Male\", \"Female\", \"Other\"],\n    col=\"start_station_name\", \n    col_wrap=3,\n    ci=\"sd\"\n);\ng.legend.remove();\ng.add_legend(title=\"Gender\");\nsns.move_legend(g, \"upper right\");\ng.set_titles(col_template=\"{col_name}\");\ng.set(xlabel=\"Day of Week\", ylabel=\"Mean Rentals\");\ng.set_xticklabels(rotation=30);"
  },
  {
    "objectID": "posts/data-viz-udacity/index.html#summary",
    "href": "posts/data-viz-udacity/index.html#summary",
    "title": "Bike Share Station Supply",
    "section": "Summary",
    "text": "Summary\nThis analysis found bike-sharing trends among days of the week, gender, and stations that may help inform decisions to ensure bike rental stations are properly stocked to meet customer demand. Based on the data, we found that we can typically expect a drop in rentals during the weekend, which may suggest that many rentals are used for work commutes. Using data for which station the rentals occurred and the gender of the renter, we are able to determine how we can expect rentals vary by day of the week for each gender and station. Using this information, we have constructed an expectation for the amount of rental traffic that must be accounted for when stocking bike stations.\nGiven these conclusions, because this data only contains rentals in the month of February, the insights found in this analysis may not hold in other months of the year. We may expect to see different results if we were to analyze a month in the summer when the weather is warmer and many of these trends may differ if we analyzed data from another city. Further analysis using the rental end times to determine net rented/returned across station may also be useful in providing more insight to help meet customer rental demand."
  },
  {
    "objectID": "posts/data-viz-eda-udacity/index.html",
    "href": "posts/data-viz-eda-udacity/index.html",
    "title": "Bike Share Data Exploration",
    "section": "",
    "text": "Data and Code are available here"
  },
  {
    "objectID": "posts/data-viz-eda-udacity/index.html#introduction",
    "href": "posts/data-viz-eda-udacity/index.html#introduction",
    "title": "Bike Share Data Exploration",
    "section": "Introduction",
    "text": "Introduction\nThe bike sharing dataset includes information about individual rides made in a bike-sharing system covering the greater San Francisco Bay area. The dataset being used in this analysis is limited to February 1, 2019 to March 1, 2019. As part of the data exploration, we will investigate various features included in this dataset in preparation for a more targeted report on the data."
  },
  {
    "objectID": "posts/data-viz-eda-udacity/index.html#preliminary-wrangling",
    "href": "posts/data-viz-eda-udacity/index.html#preliminary-wrangling",
    "title": "Bike Share Data Exploration",
    "section": "Preliminary Wrangling",
    "text": "Preliminary Wrangling\n\n# import all packages and set plots to be embedded inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\n# get the dat of week from a date time\ndef get_day_of_week(df, col):\n    df['day_of_week'] = df[col].dt.day_of_week\n    # make a list of days of week to replace the numbers (0-6) generated from the dt.day_of_week\n    days_of_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n    # replace the numbered values with the strings of the days of the week\n    df['day_of_week'] = df.day_of_week.replace(to_replace=[0, 1, 2, 3, 4, 5, 6], value=days_of_week)\n    # make the days categorical types\n    days = pd.api.types.CategoricalDtype(ordered=True, categories=days_of_week)\n    # cast the days of week column as the days categorical type\n    return df['day_of_week'].astype(days)\n\nFirst, we’ll load the data, making sure to parse the start_time and end_time as datetimes, and look at the structure of the data. There are multiple features in this dataset that are worth exploring. We can drop those that are not of any interest.\n\n# load the data and parse date columns as datetimes\nbike_raw = pd.read_csv(\"data/201902-fordgobike-tripdata.csv\", parse_dates=[\"start_time\", \"end_time\"])\n# examine the data structure\nbike_raw.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 183412 entries, 0 to 183411\nData columns (total 16 columns):\n #   Column                   Non-Null Count   Dtype         \n---  ------                   --------------   -----         \n 0   duration_sec             183412 non-null  int64         \n 1   start_time               183412 non-null  datetime64[ns]\n 2   end_time                 183412 non-null  datetime64[ns]\n 3   start_station_id         183215 non-null  float64       \n 4   start_station_name       183215 non-null  object        \n 5   start_station_latitude   183412 non-null  float64       \n 6   start_station_longitude  183412 non-null  float64       \n 7   end_station_id           183215 non-null  float64       \n 8   end_station_name         183215 non-null  object        \n 9   end_station_latitude     183412 non-null  float64       \n 10  end_station_longitude    183412 non-null  float64       \n 11  bike_id                  183412 non-null  int64         \n 12  user_type                183412 non-null  object        \n 13  member_birth_year        175147 non-null  float64       \n 14  member_gender            175147 non-null  object        \n 15  bike_share_for_all_trip  183412 non-null  object        \ndtypes: datetime64[ns](2), float64(7), int64(2), object(5)\nmemory usage: 22.4+ MB\n\n\n\n# drop the features that won't be explored\nbike = bike_raw.drop(labels=['start_station_id', 'end_station_id'], axis=1)\n\nOne feature that will be worth exploring is what day of the week each rental took place. To do that, we need to extract the day of the week from the start_time.\n\n# drop the features that won't be explored\nbike = bike_raw.drop(labels=['start_station_id', 'end_station_id'], axis=1)\n# get the day of the week from the datetime\nbike[\"day_of_week\"] = get_day_of_week(bike, \"start_time\")\n\nAnother interesting feature is which station start / end combinations are most popular. Do do that, we can create a new column containing the combination string.\n\n# concatenate the start and end stations into one string to more easily analyze the station combinations\nbike[\"station_combo\"] = bike[\"start_station_name\"] + \"/\" + bike[\"end_station_name\"]\n\n\nWhat is the structure of your dataset?\nThe data contains information on the times and locations of the bike rentals along with information on the individual renting the bike. The data is largly categorical apart from the duration of the trip and the date/time that the rent started and ended. Because this data is largely categorical, there will likely be little use of scatter plots and heavier use of bar plots.\n\n\nWhat is/are the main feature(s) of interest in your dataset?\nThe main features of interest will be looking at relationships between the the bike users, stations, and times.\n\n\nWhat features in the dataset do you think will help support your investigation into your feature(s) of interest?\nThe station names, user gender, type, start times, and duration will all be used to explore relationships between the stations, users, and times."
  },
  {
    "objectID": "posts/data-viz-eda-udacity/index.html#univariate-exploration",
    "href": "posts/data-viz-eda-udacity/index.html#univariate-exploration",
    "title": "Bike Share Data Exploration",
    "section": "Univariate Exploration",
    "text": "Univariate Exploration\n\nWhat’s the distribution of user types in the data?\nMost of the users are “Subscribers” rather than “Customers”. It’s difficult to tell from the data / documentation what the difference between these types means. Best guess is that the company allows you to subscribe to get an unlimited or X number of rentals per month as well as an option to pay for a single use of a bike.\n\nsns.countplot(data=bike, x=\"user_type\")\n\n<AxesSubplot:xlabel='user_type', ylabel='count'>\n\n\n\n\n\n\n\nWhat’s the distribution of ages in the data?\nThe birth year distribution peaks around 1990 (around 30 years old). Unsurprisingly, most of the distribution lies between 1960 and 2000 (ages 19-60). There appears to be some outliers with birth dates that are likely data entry errors. These can be cleaned up by limiting the birthdates to no earlier than 1940 (about 80 years old)\n\nsns.histplot(data=bike, x=\"member_birth_year\", bins=30)\n\n<AxesSubplot:xlabel='member_birth_year', ylabel='Count'>\n\n\n\n\n\n\n# clean the birthdates that don't make sense by removing them from the data\nbike = bike[bike['member_birth_year'] > 1940]\n\nNow that the birth years are cleaned, we can look at the histogram again\n\nsns.histplot(data=bike, x=\"member_birth_year\", bins=30)\n\n<AxesSubplot:xlabel='member_birth_year', ylabel='Count'>\n\n\n\n\n\n\n\nWhat’s the distribution of gender in the data?\nMost of the rentals in the data (over 120,000) are from males and about 4000 usages are from females. A small portion of the rentals are from users who identified as other. This data could be valuable for determining what kinds of bikes are needed to support the customer base.\n\nsns.countplot(data=bike, x=\"member_gender\")\n\n<AxesSubplot:xlabel='member_gender', ylabel='count'>\n\n\n\n\n\n\n\nWhat proportion of people are using the bike share for their entire trip?\nMost of the bike rentals did not transport the user for the entirety of their trip. It’s difficult to tell from the data/documentation if this includes walking or things like public transit and vehicles.\n\nsns.countplot(data=bike, x=\"bike_share_for_all_trip\")\n\n<AxesSubplot:xlabel='bike_share_for_all_trip', ylabel='count'>\n\n\n\n\n\n\n\nWhat is the duration of the trips made in the data?\nIt appears that the vast majority of the data is between 0 and 30 minutes (about 2000 seconds). There are outliers extending to around an hour and half.\n\nsns.histplot(data=bike, x=\"duration_sec\", bins=50, binrange=(0, 5000))\n\n<AxesSubplot:xlabel='duration_sec', ylabel='Count'>\n\n\n\n\n\n\n\nHow many people are uses are the bikes getting for each day of the week?\nIt appears that there is a higher frequency of rentals during the week (Monday - Friday) than the weekend (Saturday - Sunday). This could be due to people using the bike sharing as a way to commute to work. It could be in the interest of the bike sharing company to to know how many bikes are needed at each station for a particular day of the week givin the cyclical demand.\n\ng = sns.countplot(data=bike, y=\"day_of_week\", color=sns.color_palette()[0])\n\n\n\n\n\n\nDiscuss the distribution(s) of your variable(s) of interest. Were there any unusual points? Did you need to perform any transformations?\nThere were unusal birth years present that were cleaned by removing those rentals from the data. The variables most of interest are the gender distributions and the days of the week distribution. Those could be valuable for stocking stations for particular days.\n\n\nOf the features you investigated, were there any unusual distributions? Did you perform any operations on the data to tidy, adjust, or change the form of the data? If so, why did you do this?\nThe birth year distribution was highly skewed and included birth years that were likely data entry errors (1890). The data was cleaned by removing those observations from the dataset. It’s not possible to know the correct ages, so removal seemed to be the best option"
  },
  {
    "objectID": "posts/data-viz-eda-udacity/index.html#bivariate-exploration",
    "href": "posts/data-viz-eda-udacity/index.html#bivariate-exploration",
    "title": "Bike Share Data Exploration",
    "section": "Bivariate Exploration",
    "text": "Bivariate Exploration\n\nWhat is the distribution of durations between genders?\nIt appears that all genders have a very similar distribution, implying that there is no relationship between gender and duration time.\n\nsns.displot(data=bike, x=\"duration_sec\", bins=50, binrange=(0, 5000), kind=\"hist\", hue=\"member_gender\", alpha = .25)\n\n<seaborn.axisgrid.FacetGrid at 0x7f90ea7212b0>\n\n\n\n\n\n\n\nHow does duration differ by gender?\nAfter looking at the above distribution, we can get a clearer picture with a boxplot on whether duration varies by gender. There doesn’t appear to be any relationship between duration and gender. This plot gives a clearer picture of the lack of relationship than the histogram previously\n\nsns.catplot(data=bike, x=\"member_gender\", y=\"duration_sec\", kind=\"box\", sym=\"\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f90daaf0df0>\n\n\n\n\n\n\n\nHow does user type change over the course of a week?\nThe subscriber data follows a similar trend to the overall week distribution in the univariate section. However, the customer user type does not follow the same trend and the difference in customers between the days of the week are small enough and variable enough that there does not appear to be any interesting trends in the weekday data looking at different customer types.\n\n# facet the plots and free the y axis to get a clearer comparison of the distributions\nday_users = bike[['day_of_week', 'user_type']].value_counts().rename(\"uses\").reset_index()\ng = sns.catplot(data=day_users, x=\"day_of_week\", y=\"uses\", col='user_type', kind=\"point\", sharey=False)\ng.set_xticklabels(rotation=30);\n\n\n\n\n\n\nHow does duration time differ between customer types\nDue to the large number of outliers, it’s difficult to visualize the duration between customer types. Cutting out the outliers, it appears that customers use the bikes longer than subscribers, but the IQRs overlap, so it’s hard to say from this visualization if there is a statistically significant trend.\n\nsns.catplot(data=bike, x=\"user_type\", y=\"duration_sec\", kind=\"box\", sym=\"\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f90ea7213d0>\n\n\n\n\n\n\n\nHow does duration of a trip change with birth year of user\nThe duration of the trips were tranformed to a log scale which shows a slight trend in younger users more likely to go on longer trips.\n\nsns.scatterplot(data=bike, x=\"member_birth_year\", y=\"duration_sec\", alpha=0.25)\nplt.yscale(\"log\")\n\nticks= [100, 1000, 10000, 100000]\nlabels = ['{}'.format(v) for v in ticks]\n\nplt.yticks(ticks, labels);\n\n\n\n\n\n\nDoes birth year vary with user type?\nThere does not seem to be any trend with the user type and the birth year.\n\nsns.catplot(data=bike, x=\"user_type\", y=\"member_birth_year\", kind=\"box\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f90ea7156d0>\n\n\n\n\n\n\n\nHow many times per day are the most popular routes traveled?\nThe most popular station-to-station route, Berry St at 4th St and San Francisco Ferry Building is traveled about 14 times per day on average. The top 10 most popular routes are travelled at least 10 times per day on average for the month of data.\n\n# count the number of rentals for each station combo and keep the top 10\nstation_combos = bike.value_counts(\"station_combo\", sort=True).head(10).rename(\"num_rentals\").reset_index()\n# get the mean rentals per day (adding 1 to make it inclusive of the start/end dates)\nstation_combos['average_rentals_per_day'] = station_combos['num_rentals'] / (bike.start_time.max() - bike.start_time.min()).days + 1\nsns.barplot(data=station_combos, y=\"station_combo\", x=\"average_rentals_per_day\", color=sns.color_palette()[0])\n\n<AxesSubplot:xlabel='average_rentals_per_day', ylabel='station_combo'>\n\n\n\n\n\n\n\nWhat’s the average duration between stations?\nLooking at the stations with the largest mean duration between stations, the top ten routes have a mean duration of 10 to about 25 hours.\n\n# get the mean duration for each station combo\nstation_combo_dur = bike.groupby(\"station_combo\").duration_sec.mean().rename(\"mean_dur_secs\").reset_index().sort_values(\"mean_dur_secs\", ascending=False)\n# convert from sec to hr\nstation_combo_dur['mean_dur_hours'] = station_combo_dur['mean_dur_secs'] / 60 / 60\nsns.barplot(data=station_combo_dur.head(10), y=\"station_combo\", x=\"mean_dur_hours\", color=sns.color_palette()[0])\n\n<AxesSubplot:xlabel='mean_dur_hours', ylabel='station_combo'>\n\n\n\n\n\n\n\nTalk about some of the relationships you observed in this part of the investigation. How did the feature(s) of interest vary with other features in the dataset?\nThere doesn’t appear to be any clear trend between gender and other features, however there does appear to be a trend in member age and the duration of the bike rental. Younger users tend to rent for a longer time than older users, however, the trend is slight and a majority of users rent the bikes for around the same amount of time. There may also be a slight trend in users designated as customers having the rentals for a longer duration than the users designated as subscribers. However, the IQRs overlap, so the trend is likely not statistically significant. A hypothesis test could be conducted to determine this if we wished to investigate this further.\n\n\nDid you observe any interesting relationships between the other features (not the main feature(s) of interest)?\nWe were able to use some feature engineering to determine the number of rentals per day between station combinations. This could be further explored in combination with the lat/long data to map out the routes between stations color encoded by the average rentals per day in between each station combination. We will likely not investigate this further as it requires mapping packages."
  },
  {
    "objectID": "posts/data-viz-eda-udacity/index.html#multivariate-exploration",
    "href": "posts/data-viz-eda-udacity/index.html#multivariate-exploration",
    "title": "Bike Share Data Exploration",
    "section": "Multivariate Exploration",
    "text": "Multivariate Exploration\n\nHow does the mean number of rentals by gender, vary per day of the week for the most popular starting stations?\nUnderstanding how the number of rentals by gender varies per day of the week for the most popular stations can be useful for the company to ensure that the stations have the proper supply of the correct bike sizes to meet their customer demand.\nFirst, we can find the total number of rentals for each station start, gender, and day of the week in the dataset and filter to only look at the top six most used stations. Then, using seaborn’s catplot and using the default mean estimator, we can use num_rentals to visualize the mean and standard error.\n\n# get the date for each rental\nbike[\"date\"] = bike.start_time.dt.floor(\"D\")\n# count the number of rentals for each date, station, and gender\nbike_rentals = bike.groupby([\"start_station_name\", \"member_gender\", \"date\"]).agg(num_rentals = ('start_time', 'count')).reset_index()\n# get day of week for each date\nbike_rentals[\"day_of_week\"] = get_day_of_week(bike_rentals, \"date\")\n# get the six stations with the most total rentals\ntop_six_stations = bike_rentals.groupby(\"start_station_name\").num_rentals.sum().reset_index().sort_values(\"num_rentals\", ascending=False).head(6).start_station_name.values \n# filter the bike rentals for the top six stations\ntop_six_station_rentals = bike_rentals[bike_rentals[\"start_station_name\"].isin(top_six_stations)]\n\nWe can use a facet grid by station to see how the demand for each day of the week varies by gender. For instance, at the San Francisco Caltrain Station 2, there are a small fraction of rentals for both male and females during the weekend compared to weekdays, so that station doesn’t need to be as stocked with as many bikes as it should during the week. Whereas the Market St at 10th St station doesn’t see as large of a dropoff in female users during the weekend, so supply at that station should be about half what it is during the week to meet demand. We can also use the standard devation to get an idea of how much the demand varies for each day.\n\n# catplot defaults to the mean estimator, so using num_re\ng = sns.catplot(\n    data=top_six_station_rentals, \n    x=\"day_of_week\", y=\"num_rentals\", \n    hue=\"member_gender\", kind=\"point\",\n    hue_order=[\"Male\", \"Female\", \"Other\"],\n    col=\"start_station_name\", \n    col_wrap=3,\n    ci=\"sd\"\n);\ng.legend.remove();\ng.add_legend(title=\"Gender\");\nsns.move_legend(g, \"upper right\");\ng.set_titles(col_template=\"{col_name}\");\ng.set(xlabel=\"Day of Week\", ylabel=\"Mean Rentals\");\ng.set_xticklabels(rotation=30);\n\n\n\n\n\n\nTalk about some of the relationships you observed in this part of the investigation. Were there features that strengthened each other in terms of looking at your feature(s) of interest?\nThe main interest in this section was to create a data visualization that allows for an analysis of the mean rentals broken down by gender for each day of the week, across all stations (but only focusing on the most popular stations). The trends shown in this multivariate visualization can help inform the company on how well each bike station should be stocked to support their customer demands such as the number of bikes needed at a particular station on a given day and bike sizes to support a certain number of males and females.\n\n\nWere there any interesting or surprising interactions between features?\nAll of the top stations showed a decrease in usage during the weekend, but some stations show a much more dramatic decrease than others, suggesting that some station may be much more utilized for work commutes. This information can help determine which stations need to be supplied with fewer bikes during the week and which ones need a fairly consistent number of bikes. For example, the Powell St BART Station only sees a small decrease in rentals from females during the weekend, suggesting that their supply of bikes that are better suited for female riders should remain relatively consistent throughout the week."
  },
  {
    "objectID": "posts/data-viz-eda-udacity/index.html#conclusions",
    "href": "posts/data-viz-eda-udacity/index.html#conclusions",
    "title": "Bike Share Data Exploration",
    "section": "Conclusions",
    "text": "Conclusions\nIn this analysis, a number of relationships were examined involving the rental stations, time/duration of the rental and user information. The analysis showed that more males than females tend to rent bikes, younger users have a slightly larger tendency for longer duration rentals, and the number of rentals tends to drop during the weekends, compared to weekdays. After putting together features for station name, gender, and using feature engineering to obtain the day of the week and the mean rentals per day of the week, we were able to create a visualization that can help to inform the bike sharing company on how their bike supplies should be stocked based on the station, day of the week, and gender of their users. This information is useful for keeping stations stocked with an adequate supply to meet customer demand. Given these conclusions, because this data only contains rentals in the month of February, the insights found in this analysis may not hold in other months of the year. We may expect to see different results if we were to analyze a month in the summer when the weather is warmer and many of these trends may differ if we analyzed data from another city."
  },
  {
    "objectID": "posts/ab-test-udacity/index.html",
    "href": "posts/ab-test-udacity/index.html",
    "title": "Analyze A/B Test Results",
    "section": "",
    "text": "Introduction\nA/B tests are very commonly performed by data analysts and data scientists. For this project, you will be working to understand the results of an A/B test run by an e-commerce website. Your goal is to work through this notebook to help the company understand if they should: - Implement the new webpage, - Keep the old webpage, or - Perhaps run the experiment longer to make their decision.\n\n\nPart I - Probability\nTo get started, let’s import our libraries.\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\nrandom.seed(42)\n\n\nImport the Data\nNow, read in the ab_data.csv data. Store it in df. Below is the description of the data, there are a total of 5 columns:\n\n\n\n\n\n\n\n\n\nData columns\nPurpose\nValid values\n\n\n\n\nuser_id\nUnique ID\nInt64 values\n\n\ntimestamp\nTime stamp when the user visited the webpage\n-\n\n\ngroup\nIn the current A/B experiment, the users are categorized into two broad groups. The control group users are expected to be served with old_page; and treatment group users are matched with the new_page. However, some inaccurate rows are present in the initial data, such as a control group user is matched with a new_page.\n['control', 'treatment']\n\n\nlanding_page\nIt denotes whether the user visited the old or new webpage.\n['old_page', 'new_page']\n\n\nconverted\nIt denotes whether the user decided to pay for the company’s product. Here, 1 means yes, the user bought the product.\n[0, 1]\n\n\n\n\nUse your dataframe to answer the questions in Quiz 1 of the classroom.\na. Read in the dataset from the ab_data.csv file and take a look at the top few rows here:\n\ndf = pd.read_csv('data/ab_data.csv')\ndf.head()\n\n\n\n\n  \n    \n      \n      user_id\n      timestamp\n      group\n      landing_page\n      converted\n    \n  \n  \n    \n      0\n      851104\n      2017-01-21 22:11:48.556739\n      control\n      old_page\n      0\n    \n    \n      1\n      804228\n      2017-01-12 08:01:45.159739\n      control\n      old_page\n      0\n    \n    \n      2\n      661590\n      2017-01-11 16:55:06.154213\n      treatment\n      new_page\n      0\n    \n    \n      3\n      853541\n      2017-01-08 18:28:03.143765\n      treatment\n      new_page\n      0\n    \n    \n      4\n      864975\n      2017-01-21 01:52:26.210827\n      control\n      old_page\n      1\n    \n  \n\n\n\n\nb. Use the cell below to find the number of rows in the dataset.\n\ndf.shape\n\n(294478, 5)\n\n\nc. The number of unique users in the dataset.\n\ndf.user_id.nunique()\n\n290584\n\n\nd. The proportion of users converted.\n\ndf.converted.mean()\n\n0.11965919355605512\n\n\ne. The number of times when the “group” is treatment but “landing_page” is not a new_page.\n\nwrong_treatment = ((df['group'] == 'treatment') & (df['landing_page'] != 'new_page')).sum()\n\n\nwrong_control = ((df['group'] == 'control') & (df['landing_page'] != 'old_page')).sum()\n\n\nwrong_treatment + wrong_control\n\n3893\n\n\nf. Do any of the rows have missing values?\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 294478 entries, 0 to 294477\nData columns (total 5 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   user_id       294478 non-null  int64 \n 1   timestamp     294478 non-null  object\n 2   group         294478 non-null  object\n 3   landing_page  294478 non-null  object\n 4   converted     294478 non-null  int64 \ndtypes: int64(2), object(3)\nmemory usage: 11.2+ MB\n\n\nNo missing values.\n\n\nClean the Data\nIn a particular row, the group and landing_page columns should have either of the following acceptable values:\n\n\n\nuser_id\ntimestamp\ngroup\nlanding_page\nconverted\n\n\n\n\nXXXX\nXXXX\ncontrol\nold_page\nX\n\n\nXXXX\nXXXX\ntreatment\nnew_page\nX\n\n\n\nIt means, the control group users should match with old_page; and treatment group users should matched with the new_page.\nHowever, for the rows where treatment does not match with new_page or control does not match with old_page, we cannot be sure if such rows truly received the new or old wepage.\nUse Quiz 2 in the classroom to figure out how should we handle the rows where the group and landing_page columns don’t match?\na. Now use the answer to the quiz to create a new dataset that meets the specifications from the quiz. Store your new dataframe in df2.\n\n# Remove the inaccurate rows, and store the result in a new dataframe df2\ndf2 = df[((df['group'] == 'treatment') & (df['landing_page'] == 'new_page')) |\n          ((df['group'] == 'control') & (df['landing_page'] == 'old_page'))]\n\n\n# Double Check all of the incorrect rows were removed from df2 - \n# Output of the statement below should be 0\nwrong_treatment2 = ((df2['group'] == 'treatment') & (df2['landing_page'] != 'new_page')).sum()\nwrong_control2 = ((df2['group'] == 'control') & (df2['landing_page'] != 'old_page')).sum()\nwrong_treatment2 + wrong_control2\n\n0\n\n\nUse df2 and the cells below to answer questions for Quiz 3 in the classroom.\na. How many unique user_ids are in df2?\n\ndf2.shape\n\n(290585, 5)\n\n\n\ndf2.user_id.nunique()\n\n290584\n\n\nb. There is one user_id repeated in df2. What is it?\n\ndf2.user_id.value_counts().head()\n\n773192    2\n851104    1\n688307    1\n718297    1\n838144    1\nName: user_id, dtype: int64\n\n\nc. Display the rows for the duplicate user_id?\n\ndf2[df2['user_id'] == 773192]\n\n\n\n\n  \n    \n      \n      user_id\n      timestamp\n      group\n      landing_page\n      converted\n    \n  \n  \n    \n      1899\n      773192\n      2017-01-09 05:37:58.781806\n      treatment\n      new_page\n      0\n    \n    \n      2893\n      773192\n      2017-01-14 02:55:59.590927\n      treatment\n      new_page\n      0\n    \n  \n\n\n\n\nd. Remove one of the rows with a duplicate user_id, from the df2 dataframe.\n\n# Remove one of the rows with a duplicate user_id..\n# Hint: The dataframe.drop_duplicates() may not work in this case because the rows with duplicate user_id are not entirely identical.\n# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html\ndf2.drop_duplicates(subset='user_id', inplace=True)\n# Check again if the row with a duplicate user_id is deleted or not\ndf2.user_id.value_counts().head()\n\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_92818/377076188.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df2.drop_duplicates(subset='user_id', inplace=True)\n\n\n851104    1\n688307    1\n718297    1\n838144    1\n728209    1\nName: user_id, dtype: int64\n\n\n\ndf2.shape\n\n(290584, 5)\n\n\n\n\nCalculate Click Through Rates\nUse df2 in the cells below to answer the quiz questions related to Quiz 4 in the classroom.\na. What is the probability of an individual converting regardless of the page they receive?\n\ndf2.converted.mean()\n\n0.11959708724499628\n\n\nb. Given that an individual was in the control group, what is the probability they converted?\n\ncontrol_cr = df2[df['group'] == 'control'].converted.mean()\ncontrol_cr\n\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_92818/2805132061.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  control_cr = df2[df['group'] == 'control'].converted.mean()\n\n\n0.1203863045004612\n\n\nc. Given that an individual was in the treatment group, what is the probability they converted?\n\ntreatment_cr = df2[df['group'] == 'treatment'].converted.mean()\ntreatment_cr\n\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_92818/1278796718.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  treatment_cr = df2[df['group'] == 'treatment'].converted.mean()\n\n\n0.11880806551510564\n\n\n\n# Calculate the actual difference (obs_diff) between the conversion rates for the two groups.\nobs_diff = treatment_cr - control_cr\nobs_diff\n\n-0.0015782389853555567\n\n\nd. What is the probability that an individual received the new page?\n\n(df2['landing_page'] == 'new_page').mean()\n\n0.5000619442226688\n\n\ne. Consider your results from parts (a) through (d) above, and explain below whether the new treatment group users lead to more conversions.\nBased on the conversion rates for the treatment and the control groups, there seemed to be a lower conversion rate for the treatment group than the control group, indicating that the new page may not lead to more purchases than the old page.\n\n\n\nPart II - A/B Test\nSince a timestamp is associated with each event, you could run a hypothesis test continuously as long as you observe the events.\nHowever, then the hard questions would be: - Do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?\n- How long do you run to render a decision that neither page is better than another?\nThese questions are the difficult parts associated with A/B tests in general.\n\nHypothesis Test Setup\nFor now, consider you need to make the decision just based on all the data provided.\n\nRecall that you just calculated that the “converted” probability (or rate) for the old page is slightly higher than that of the new page (ToDo 1.4.c).\n\nIf you want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, what should be your null and alternative hypotheses (\\(H_0\\) and \\(H_1\\))?\nYou can state your hypothesis in terms of words or in terms of \\(p_{old}\\) and \\(p_{new}\\), which are the “converted” probability (or rate) for the old and new pages respectively.\nThe Null Hypothesis is that the new page leads to as many or less conversions than the old page. The Alternative Hypothesis is that the new page leads to more conversions than the old page.\n\\[H_0: p_{new} - p_{old} \\leq 0\\] \\[H_1: p_{new} - p_{old} > 0\\]\n\n\nNull Hypothesis \\(H_0\\) Testing\nUnder the null hypothesis \\(H_0\\), assume that \\(p_{new}\\) and \\(p_{old}\\) are equal. Furthermore, assume that \\(p_{new}\\) and \\(p_{old}\\) both are equal to the converted success rate in the df2 data regardless of the page. So, our assumption is: \n\n\\(p_{new}\\) = \\(p_{old}\\) = \\(p_{population}\\)\n\nIn this section, you will:\n\nSimulate (bootstrap) sample data set for both groups, and compute the “converted” probability \\(p\\) for those samples.\nUse a sample size for each group equal to the ones in the df2 data.\nCompute the difference in the “converted” probability for the two samples above.\nPerform the sampling distribution for the “difference in the converted probability” between the two simulated-samples over 10,000 iterations; and calculate an estimate.\n\nUse the cells below to provide the necessary parts of this simulation. You can use Quiz 5 in the classroom to make sure you are on the right track.\na. What is the conversion rate for \\(p_{new}\\) under the null hypothesis?\nBecause our Null hypothesis assumes that \\(p_{new} = p_{old}\\), the conversion rate for the new and old pages should be equal to the entire conversion rate in the data, therefore being the same.\n\nconv_rate_new = df2.converted.mean()\nconv_rate_new\n\n0.11959708724499628\n\n\nb. What is the conversion rate for \\(p_{old}\\) under the null hypothesis?\n\nconv_rate_old = df2.converted.mean()\nconv_rate_old\n\n0.11959708724499628\n\n\n\nconv_rate_new - conv_rate_old\n\n0.0\n\n\nc. What is \\(n_{new}\\), the number of individuals in the treatment group?  Hint: The treatment group users are shown the new page.\n\ntreatment_rows = df2[df2['group'] == 'treatment'].shape[0]\ntreatment_rows\n\n145310\n\n\nd. What is \\(n_{old}\\), the number of individuals in the control group?\n\ncontrol_rows = df2[df2['group'] == 'control'].shape[0]\ncontrol_rows\n\n145274\n\n\ne. Simulate Sample for the treatment Group Simulate \\(n_{new}\\) transactions with a conversion rate of \\(p_{new}\\) under the null hypothesis. \n\n# Simulate a Sample for the treatment Group\n# 0 or 1 outcome for the number of people who got the treatment\ntreatment_sample = np.random.choice([0, 1], size=treatment_rows, p=[1-conv_rate_new, conv_rate_new])\n\nf. Simulate Sample for the control Group  Simulate \\(n_{old}\\) transactions with a conversion rate of \\(p_{old}\\) under the null hypothesis.  Store these \\(n_{old}\\) 1’s and 0’s in the old_page_converted numpy array.\n\n# Simulate a Sample for the control Group\ncontrol_sample = np.random.choice([0, 1], size=control_rows, p=[1-conv_rate_old, conv_rate_old])\n\ng. Find the difference in the “converted” probability \\((p{'}_{new}\\) - \\(p{'}_{old})\\) for your simulated samples from the parts (e) and (f) above.\n\ntreatment_sample.mean() - control_sample.mean()\n\n8.744910259526895e-05\n\n\nh. Sampling distribution  Re-create new_page_converted and old_page_converted and find the \\((p{'}_{new}\\) - \\(p{'}_{old})\\) value 10,000 times using the same simulation process you used in parts (a) through (g) above.\n Store all \\((p{'}_{new}\\) - \\(p{'}_{old})\\) values in a NumPy array called p_diffs.\n\n# sampling distribution\n# use the binomal function (because we are sampling 1s or 0s) to take advantage of \n# the function speed over looping/sampling\n\n# returns the number number of treatment rows that converted (based on the conversion rate and binomical dist) for\n# 10,000 samples and divides by the number of treatment rows to get the pct conversion for the samples\ntreatment_sample = np.random.binomial(treatment_rows, conv_rate_new, 10000) / treatment_rows\ncontrol_sample = np.random.binomial(control_rows, conv_rate_old, 10000) / control_rows\np_diffs = treatment_sample - control_sample\n\ni. Histogram Plot a histogram of the p_diffs. Does this plot look like what you expected? Use the matching problem in the classroom to assure you fully understand what was computed here.\nAlso, use plt.axvline() method to mark the actual difference observed in the df2 data (recall obs_diff), in the chart.\n\nplt.hist(p_diffs);\nplt.xlabel(\"conversion rate probability difference\")\nplt.ylabel(\"Count\")\nplt.title(\"Conversion Rates Sample from the Null\")\nplt.axvline(obs_diff, color='red')\n\n<matplotlib.lines.Line2D at 0x7ff0ba67b5e0>\n\n\n\n\n\nj. What proportion of the p_diffs are greater than the actual difference observed in the df2 data?\n\n(p_diffs > obs_diff).mean()\n\n0.9055\n\n\nk. Please explain in words what you have just computed in part j above.\n- What is this value called in scientific studies?\n- What does this value signify in terms of whether or not there is a difference between the new and old pages? Hint: Compare the value above with the “Type I error rate (0.05)”.\nThe p-value calculated above of 0.902, considering a Type I error rate of (0.05) clearly shows that we cannot reject the null hypothesis that the old page has a conversion equal to or greater than the new page. By bootstrap sampling from a Null that the two pages had equal conversion rates, we can see from the historgram above and the p-value that the observed conversion rate is much less than large majority of the null distribution. Because the p-value and its extreme toward the alternative hypothesis (greater than the null), contain most of the null distribution, we do not have evidence to reject the null hypothesis.\nl. Using Built-in Methods for Hypothesis Testing We could also use a built-in to achieve similar results. Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance.\nFill in the statements below to calculate the: - convert_old: number of conversions with the old_page - convert_new: number of conversions with the new_page - n_old: number of individuals who were shown the old_page - n_new: number of individuals who were shown the new_page\n\nimport statsmodels.api as sm\n\n# number of conversions with the old_page\nconvert_old = df2[df2['landing_page'] == \"old_page\"].converted.sum()\n\n# number of conversions with the new_page\nconvert_new = df2[df2['landing_page'] == \"new_page\"].converted.sum()\n\n# number of individuals who were shown the old_page\nn_old = df2[df2['landing_page'] == \"old_page\"].shape[0]\n\n# number of individuals who received new_page\nn_new = df2[df2['landing_page'] == \"new_page\"].shape[0]\n\nm. Now use sm.stats.proportions_ztest() to compute your test statistic and p-value. Here is a helpful link on using the built in.\nThe syntax is:\nproportions_ztest(count_array, nobs_array, alternative='larger')\nwhere, - count_array = represents the number of “converted” for each group - nobs_array = represents the total number of observations (rows) in each group - alternative = choose one of the values from [‘two-sided’, ‘smaller’, ‘larger’] depending upon two-tailed, left-tailed, or right-tailed respectively.\nThe built-in function above will return the z_score, p_value.\n\n\nAbout the two-sample z-test\nRecall that you have plotted a distribution p_diffs representing the difference in the “converted” probability \\((p{'}_{new}-p{'}_{old})\\) for your two simulated samples 10,000 times.\nAnother way for comparing the mean of two independent and normal distribution is a two-sample z-test. You can perform the Z-test to calculate the Z_score, as shown in the equation below:\n\\[Z_{score} = \\frac{ (p{'}_{new}-p{'}_{old}) - (p_{new}  -  p_{old})}{ \\sqrt{ \\frac{\\sigma^{2}_{new} }{n_{new}} + \\frac{\\sigma^{2}_{old} }{n_{old}}  } }\\]\nwhere, - \\(p{'}\\) is the “converted” success rate in the sample - \\(p_{new}\\) and \\(p_{old}\\) are the “converted” success rate for the two groups in the population. - \\(\\sigma_{new}\\) and \\(\\sigma_{new}\\) are the standard deviation for the two groups in the population. - \\(n_{new}\\) and \\(n_{old}\\) represent the size of the two groups or samples (it’s same in our case)\n\nZ-test is performed when the sample size is large, and the population variance is known. The z-score represents the distance between the two “converted” success rates in terms of the standard error.\n\nNext step is to make a decision to reject or fail to reject the null hypothesis based on comparing these two values: - \\(Z_{score}\\) - \\(Z_{\\alpha}\\) or \\(Z_{0.05}\\), also known as critical value at 95% confidence interval. \\(Z_{0.05}\\) is 1.645 for one-tailed tests, and 1.960 for two-tailed test. You can determine the \\(Z_{\\alpha}\\) from the z-table manually.\nDecide if your hypothesis is either a two-tailed, left-tailed, or right-tailed test. Accordingly, reject OR fail to reject the null based on the comparison between \\(Z_{score}\\) and \\(Z_{\\alpha}\\). We determine whether or not the \\(Z_{score}\\) lies in the “rejection region” in the distribution. In other words, a “rejection region” is an interval where the null hypothesis is rejected iff the \\(Z_{score}\\) lies in that region.\nReference: - Example 9.1.2 on this page, courtesy www.stats.libretexts.org\n\nimport statsmodels.api as sm\n# ToDo: Complete the sm.stats.proportions_ztest() method arguments\nz_score, p_value = sm.stats.proportions_ztest(\n    [convert_new, convert_old],\n    [n_new, n_old],\n    alternative='larger')\nprint(z_score, p_value)\n\n-1.3109241984234394 0.9050583127590245\n\n\nn. What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages? Do they agree with the findings in parts j. and k.?\nThe p-value is very close to (and effectively the same as) the p-value calculated previously. Therefore, this p-value does indicate the same result that the null hypothesis cannot be rejected. \n\n\n\nPart III - A regression approach\n\nInterential Logistic Regression\nIn this final part, you will see that the result you achieved in the A/B test in Part II above can also be achieved by performing regression.\na. Since each row in the df2 data is either a conversion or no conversion, what type of regression should you be performing in this case?\nBecause it is binary (2 choices), logistic regression should be used.\nb. The goal is to use statsmodels library to fit the regression model you specified in part a. above to see if there is a significant difference in conversion based on the page-type a customer receives. However, you first need to create the following two columns in the df2 dataframe: 1. intercept - It should be 1 in the entire column. 2. ab_page - It’s a dummy variable column, having a value 1 when an individual receives the treatment, otherwise 0.\n\ndf2['intercept'] = 1\ndf2[['ab_page', 'old_page']]=pd.get_dummies(df['landing_page'])\ndf2.drop('old_page', axis=1, inplace=True)\n\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_92818/2238491058.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df2['intercept'] = 1\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_92818/2238491058.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df2[['ab_page', 'old_page']]=pd.get_dummies(df['landing_page'])\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_92818/2238491058.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df2[['ab_page', 'old_page']]=pd.get_dummies(df['landing_page'])\n/var/folders/hm/76vk5x9156vcx4df2s8mw2hm0000gn/T/ipykernel_92818/2238491058.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df2.drop('old_page', axis=1, inplace=True)\n\n\nc. Use statsmodels to instantiate your regression model on the two columns you created in part (b). above, then fit the model to predict whether or not an individual converts.\n\nlogit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\nresult = logit_mod.fit()\n\nOptimization terminated successfully.\n         Current function value: 0.366118\n         Iterations 6\n\n\nd. Provide the summary of your model below, and use it as necessary to answer the following questions.\n\nresult.summary2()\n\n\n\n        Model:              Logit      Pseudo R-squared:    0.000   \n\n\n  Dependent Variable:     converted          AIC:        212780.3502\n\n\n         Date:        2022-05-22 16:49       BIC:        212801.5095\n\n\n   No. Observations:       290584       Log-Likelihood:  -1.0639e+05\n\n\n       Df Model:              1            LL-Null:      -1.0639e+05\n\n\n     Df Residuals:         290582        LLR p-value:      0.18988  \n\n\n      Converged:           1.0000           Scale:         1.0000   \n\n\n    No. Iterations:        6.0000                                   \n\n\n\n\n             Coef.  Std.Err.     z      P>|z| [0.025  0.975] \n\n\n  intercept -1.9888  0.0081  -246.6690 0.0000 -2.0046 -1.9730\n\n\n  ab_page   -0.0150  0.0114   -1.3109  0.1899 -0.0374 0.0074 \n\n\n\n\ne. What is the p-value associated with ab_page? Why does it differ from the value you found in Part II?\nThe null hypothsis in the regression model is that the coefficient has a value of 0. This says that there is no relationship between the pages and converting. This is slightly different from the null hypothesis from the A/B test because it implies a two-sided test as opposed to a one-sided test in the hypothesis test from Part II.\nf. Now, you are considering other things that might influence whether or not an individual converts. Discuss why it is a good idea to consider other factors to add into your regression model. Are there any disadvantages to adding additional terms into your regression model?\nOther factors you could look at are how the time of day or location affect whether a user converts. The other question to ask is whether the test ran for a long enough time so that returning customers could adjust to the new page layout. One disadvantage that could result from adding more explantory variables to a linear model is multicolinearity. If two of the explanatory variables are also correlated, this can cause strange values in the coefficients to show up, leading to incorrect interpretations.\ng. Adding countries Now along with testing if the conversion rate changes for different pages, also add an effect based on which country a user lives in.\n\nYou will need to read in the countries.csv dataset and merge together your df2 datasets on the appropriate rows. You call the resulting dataframe df_merged. Here are the docs for joining tables.\nDoes it appear that country had an impact on conversion? To answer this question, consider the three unique values, ['UK', 'US', 'CA'], in the country column. Create dummy variables for these country columns.\n\nProvide the statistical output as well as a written response to answer this question.\n\n# Read the countries.csv\ncountries = pd.read_csv('data/countries.csv')\n\n\n# Join with the df2 dataframe\ndf2= df2.set_index('user_id').join(countries.set_index('user_id'), how='inner', on='user_id', lsuffix='df2', rsuffix='countries')\n\n\n# Create the necessary dummy variables\ndf2['intercept'] = 1\ndf2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country'])\n\n\n# Fit your model, and summarize the results\nlogit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'UK', 'CA']])\nresult = logit_mod.fit()\nresult.summary2()\n\nOptimization terminated successfully.\n         Current function value: 0.366113\n         Iterations 6\n\n\n\n\n        Model:              Logit      Pseudo R-squared:    0.000   \n\n\n  Dependent Variable:     converted          AIC:        212781.1253\n\n\n         Date:        2022-05-22 16:49       BIC:        212823.4439\n\n\n   No. Observations:       290584       Log-Likelihood:  -1.0639e+05\n\n\n       Df Model:              3            LL-Null:      -1.0639e+05\n\n\n     Df Residuals:         290580        LLR p-value:      0.17599  \n\n\n      Converged:           1.0000           Scale:         1.0000   \n\n\n    No. Iterations:        6.0000                                   \n\n\n\n\n             Coef.  Std.Err.     z      P>|z| [0.025  0.975] \n\n\n  intercept -1.9893  0.0089  -223.7628 0.0000 -2.0067 -1.9718\n\n\n  ab_page   -0.0149  0.0114   -1.3069  0.1912 -0.0374 0.0075 \n\n\n  UK        0.0099   0.0133   0.7433   0.4573 -0.0162 0.0359 \n\n\n  CA        -0.0408  0.0269   -1.5161  0.1295 -0.0934 0.0119 \n\n\n\n\n\n1/np.exp(-.0149)\n\n1.0150115583846535\n\n\n\nnp.exp(.0099)\n\n1.0099491671175422\n\n\n\n1/np.exp(-0.408)\n\n1.5038071611701118\n\n\nBased on the logistic regression results above, we cannot reject the null hypothesis for any of the explanatory variables. Based on the results, the old page more 1.015 times effective at converting users, users in Canada are 1.01 times less likely to convert to the convert compared to US customers and UK customers are 1.5 times more likely to convert compared to US customers. However, the p-values are too large to convince us that there is a statistically significat correlation in effect here. The differences are small and since we did not have evidence to reject the null hypothesis, it would be best to stay with the old page.\nh. Fit your model and obtain the results Though you have now looked at the individual factors of country and page on conversion, we would now like to look at an interaction between page and country to see if are there significant effects on conversion. Create the necessary additional columns, and fit the new model.\nProvide the summary results (statistical output), and your conclusions (written response) based on the results.\n\ndf2['UK_abpage'] = df2['UK'] * df2['ab_page']\ndf2['CA_abpage'] = df2['CA'] * df2['ab_page']\n\n\n# Fit your model, and summarize the results\nlogit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'UK', 'CA', 'UK_abpage', 'CA_abpage']])\nresult = logit_mod.fit()\nresult.summary2()\n\nOptimization terminated successfully.\n         Current function value: 0.366109\n         Iterations 6\n\n\n\n\n        Model:              Logit      Pseudo R-squared:    0.000   \n\n\n  Dependent Variable:     converted          AIC:        212782.6602\n\n\n         Date:        2022-05-22 16:49       BIC:        212846.1381\n\n\n   No. Observations:       290584       Log-Likelihood:  -1.0639e+05\n\n\n       Df Model:              5            LL-Null:      -1.0639e+05\n\n\n     Df Residuals:         290578        LLR p-value:      0.19199  \n\n\n      Converged:           1.0000           Scale:         1.0000   \n\n\n    No. Iterations:        6.0000                                   \n\n\n\n\n             Coef.  Std.Err.     z      P>|z| [0.025  0.975] \n\n\n  intercept -1.9865  0.0096  -206.3440 0.0000 -2.0053 -1.9676\n\n\n  ab_page   -0.0206  0.0137   -1.5052  0.1323 -0.0473 0.0062 \n\n\n  UK        -0.0057  0.0188   -0.3057  0.7598 -0.0426 0.0311 \n\n\n  CA        -0.0175  0.0377   -0.4652  0.6418 -0.0914 0.0563 \n\n\n  UK_abpage 0.0314   0.0266   1.1807   0.2377 -0.0207 0.0835 \n\n\n  CA_abpage -0.0469  0.0538   -0.8718  0.3833 -0.1523 0.0585 \n\n\n\n\n\n1/np.exp(-.0206)\n\n1.020813644503746\n\n\n\n1/np.exp(-.0057)\n\n1.0057162759095335\n\n\n\n1/np.exp(-0.175)\n\n1.191246216612358\n\n\nUsing the interactions, the old page shows a 1.02 times higher conversion than the new page. However, both the UK and CA now show less conversion than the US (UK 1.005 times less, CA 1.19 times less). For the UK/Page interaction, there is a positive correlation with conversion and a negative correlation for the CA/Page interaction, however, because these are higher order terms, they are more difficult to use for inference compared to the others where we can clearly see the effect it is having on the conversion. However as with the case above, the p-values are too large to be convinced of any statistical significance in the correlation.\n\n\n\nConclusion\nIn this analysis, multiple techniques were utilized to determine whether a new landing page should be used in lieu of the old page. The results of the hypothesis test showed a p-value of 0.9, indicating that we cannot reject the null hypothesis that the old page leads to just as many, if not more conversions than the new page. Using the data to build a logistic regression model also showed high p-values, hinting that the correlations found in the model are not statistically significant. The recommendation resulting from this analysis to to continue use of the old page rather than converting to the new page."
  },
  {
    "objectID": "posts/finding-donars-udacity/index.html",
    "href": "posts/finding-donars-udacity/index.html",
    "title": "Finding Donors for CharityML",
    "section": "",
    "text": "Link to Github repository\n#Project: Finding Donors for CharityML\nWelcome to the first project of the Intro to Machine Learning Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with ‘Implementation’ in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully!\nIn addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a ‘Question X’ header. Carefully read each question and provide thorough answers in the following text boxes that begin with ‘Answer:’. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide."
  },
  {
    "objectID": "posts/finding-donars-udacity/index.html#getting-started",
    "href": "posts/finding-donars-udacity/index.html#getting-started",
    "title": "Finding Donors for CharityML",
    "section": "Getting Started",
    "text": "Getting Started\nIn this project, you will employ several supervised algorithms of your choice to accurately model individuals’ income using data collected from the 1994 U.S. Census. You will then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. Your goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations. Understanding an individual’s income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with. While it can be difficult to determine an individual’s general income bracket directly from public sources, we can (as we will see) infer this value from other publically available features.\nThe dataset for this project originates from the UCI Machine Learning Repository. The datset was donated by Ron Kohavi and Barry Becker, after being published in the article “Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid”. You can find the article by Ron Kohavi online. The data we investigate here consists of small changes to the original dataset, such as removing the 'fnlwgt' feature and records with missing or ill-formatted entries."
  },
  {
    "objectID": "posts/finding-donars-udacity/index.html#exploring-the-data",
    "href": "posts/finding-donars-udacity/index.html#exploring-the-data",
    "title": "Finding Donors for CharityML",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nRun the code cell below to load necessary Python libraries and load the census data. Note that the last column from this dataset, 'income', will be our target label (whether an individual makes more than, or at most, $50,000 annually). All other columns are features about each individual in the census database.\n\n# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nfrom time import time\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\n# Import supplementary visualization code visuals.py\nimport visuals as vs\n\n# Pretty display for notebooks\n%matplotlib inline\n\n# Load the Census dataset\ndata = pd.read_csv(\"census.csv\")\n\n# Success - Display the first record\ndisplay(data.head(n=1))\n\n\n\n\n  \n    \n      \n      age\n      workclass\n      education_level\n      education-num\n      marital-status\n      occupation\n      relationship\n      race\n      sex\n      capital-gain\n      capital-loss\n      hours-per-week\n      native-country\n      income\n    \n  \n  \n    \n      0\n      39\n      State-gov\n      Bachelors\n      13.0\n      Never-married\n      Adm-clerical\n      Not-in-family\n      White\n      Male\n      2174.0\n      0.0\n      40.0\n      United-States\n      <=50K\n    \n  \n\n\n\n\n\nImplementation: Data Exploration\nA cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than $50,000. In the code cell below, you will need to compute the following: - The total number of records, 'n_records' - The number of individuals making more than $50,000 annually, 'n_greater_50k'. - The number of individuals making at most $50,000 annually, 'n_at_most_50k'. - The percentage of individuals making more than $50,000 annually, 'greater_percent'.\n** HINT: ** You may need to look at the table above to understand how the 'income' entries are formatted.\n\ndata[\"income\"].value_counts()\n\n<=50K    34014\n>50K     11208\nName: income, dtype: int64\n\n\n\n# TODO: Total number of records\nn_records = data.shape[0]\n\n# TODO: Number of records where individual's income is more than $50,000\nn_greater_50k = data[data[\"income\"] == \">50K\"].shape[0]\n\n# TODO: Number of records where individual's income is at most $50,000\nn_at_most_50k = data[data[\"income\"] == \"<=50K\"].shape[0]\n\n# TODO: Percentage of individuals whose income is more than $50,000\ngreater_percent = np.round((data[\"income\"] == \">50K\").mean() * 100, 2)\n\n# Print the results\nprint(\"Total number of records: {}\".format(n_records))\nprint(\"Individuals making more than $50,000: {}\".format(n_greater_50k))\nprint(\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\nprint(\"Percentage of individuals making more than $50,000: {}%\".format(greater_percent))\n\nTotal number of records: 45222\nIndividuals making more than $50,000: 11208\nIndividuals making at most $50,000: 34014\nPercentage of individuals making more than $50,000: 24.78%\n\n\nFeatureset Exploration\n\nage: continuous.\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\neducation-num: continuous.\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\nrace: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other.\nsex: Female, Male.\ncapital-gain: continuous.\ncapital-loss: continuous.\nhours-per-week: continuous.\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
  },
  {
    "objectID": "posts/finding-donars-udacity/index.html#preparing-the-data",
    "href": "posts/finding-donars-udacity/index.html#preparing-the-data",
    "title": "Finding Donors for CharityML",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nBefore data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured — this is typically known as preprocessing. Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms.\n\nTransforming Skewed Continuous Features\nA dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number. Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. With the census dataset two features fit this description: ’capital-gain' and 'capital-loss'.\nRun the code cell below to plot a histogram of these two features. Note the range of the values present and how they are distributed.\n\n# Split the data into features and target label\nincome_raw = data['income']\nfeatures_raw = data.drop('income', axis = 1)\n\n# Visualize skewed continuous features of original data\nvs.distribution(data)\n\n/Users/seanhoyt/Documents/Documents - Sean’s MacBook Pro/repos/website/posts/finding-donars-udacity/visuals.py:48: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\nFor highly-skewed feature distributions such as 'capital-gain' and 'capital-loss', it is common practice to apply a logarithmic transformation on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of 0 is undefined, so we must translate the values by a small amount above 0 to apply the the logarithm successfully.\nRun the code cell below to perform a transformation on the data and visualize the results. Again, note the range of values and how they are distributed.\n\n# Log-transform the skewed features\nskewed = ['capital-gain', 'capital-loss']\nfeatures_log_transformed = pd.DataFrame(data = features_raw)\nfeatures_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n\n# Visualize the new log distributions\nvs.distribution(features_log_transformed, transformed = True)\n\n/Users/seanhoyt/Documents/Documents - Sean’s MacBook Pro/repos/website/posts/finding-donars-udacity/visuals.py:48: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n\n\nNormalizing Numerical Features\nIn addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature’s distribution (such as 'capital-gain' or 'capital-loss' above); however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below.\nRun the code cell below to normalize each numerical feature. We will use sklearn.preprocessing.MinMaxScaler for this.\n\n# Import sklearn.preprocessing.StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\nnumerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n\nfeatures_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\nfeatures_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(features_log_minmax_transform.head(n = 5))\n\n\n\n\n  \n    \n      \n      age\n      workclass\n      education_level\n      education-num\n      marital-status\n      occupation\n      relationship\n      race\n      sex\n      capital-gain\n      capital-loss\n      hours-per-week\n      native-country\n    \n  \n  \n    \n      0\n      0.301370\n      State-gov\n      Bachelors\n      0.800000\n      Never-married\n      Adm-clerical\n      Not-in-family\n      White\n      Male\n      0.667492\n      0.0\n      0.397959\n      United-States\n    \n    \n      1\n      0.452055\n      Self-emp-not-inc\n      Bachelors\n      0.800000\n      Married-civ-spouse\n      Exec-managerial\n      Husband\n      White\n      Male\n      0.000000\n      0.0\n      0.122449\n      United-States\n    \n    \n      2\n      0.287671\n      Private\n      HS-grad\n      0.533333\n      Divorced\n      Handlers-cleaners\n      Not-in-family\n      White\n      Male\n      0.000000\n      0.0\n      0.397959\n      United-States\n    \n    \n      3\n      0.493151\n      Private\n      11th\n      0.400000\n      Married-civ-spouse\n      Handlers-cleaners\n      Husband\n      Black\n      Male\n      0.000000\n      0.0\n      0.397959\n      United-States\n    \n    \n      4\n      0.150685\n      Private\n      Bachelors\n      0.800000\n      Married-civ-spouse\n      Prof-specialty\n      Wife\n      Black\n      Female\n      0.000000\n      0.0\n      0.397959\n      Cuba\n    \n  \n\n\n\n\n\n\nImplementation: Data Preprocessing\nFrom the table in Exploring the Data above, we can see there are several features for each record that are non-numeric. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called categorical variables) be converted. One popular way to convert categorical variables is by using the one-hot encoding scheme. One-hot encoding creates a “dummy” variable for each possible category of each non-numeric feature. For example, assume someFeature has three possible entries: A, B, or C. We then encode this feature into someFeature_A, someFeature_B and someFeature_C.\n  | someFeature | | someFeature_A | someFeature_B | someFeature_C |\n:-: | :-: | | :-: | :-: | :-: |\n0 | B | | 0 | 1 | 0 |\n1 | C | —-> one-hot encode —-> | 0 | 0 | 1 |\n2 | A | | 1 | 0 | 0 |\nAdditionally, as with the non-numeric features, we need to convert the non-numeric target label, 'income' to numerical values for the learning algorithm to work. Since there are only two possible categories for this label (“<=50K” and “>50K”), we can avoid using one-hot encoding and simply encode these two categories as 0 and 1, respectively. In code cell below, you will need to implement the following: - Use pandas.get_dummies() to perform one-hot encoding on the 'features_log_minmax_transform' data. - Convert the target label 'income_raw' to numerical entries. - Set records with “<=50K” to 0 and records with “>50K” to 1.\n\n# TODO: One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\nfeatures_final = pd.get_dummies(features_log_minmax_transform)\n\n# TODO: Encode the 'income_raw' data to numerical values\nincome = pd.get_dummies(data[\"income\"])[\">50K\"]\n\n# Print the number of features after one-hot encoding\nencoded = list(features_final.columns)\nprint(\"{} total features after one-hot encoding.\".format(len(encoded)))\n\n# Uncomment the following line to see the encoded feature names\n# print(encoded)\n\n103 total features after one-hot encoding.\n\n\n\n\nShuffle and Split Data\nNow all categorical variables have been converted into numerical features, and all numerical features have been normalized. As always, we will now split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing.\nRun the code cell below to perform this split.\n\n# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Split the 'features' and 'income' data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features_final, \n                                                    income, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))\n\nTraining set has 36177 samples.\nTesting set has 9045 samples."
  },
  {
    "objectID": "posts/finding-donars-udacity/index.html#evaluating-model-performance",
    "href": "posts/finding-donars-udacity/index.html#evaluating-model-performance",
    "title": "Finding Donors for CharityML",
    "section": "Evaluating Model Performance",
    "text": "Evaluating Model Performance\nIn this section, we will investigate four different algorithms, and determine which is best at modeling the data. Three of these algorithms will be supervised learners of your choice, and the fourth algorithm is known as a naive predictor.\n\nMetrics and the Naive Predictor\nCharityML, equipped with their research, knows individuals that make more than $50,000 are most likely to donate to their charity. Because of this, CharityML is particularly interested in predicting who makes more than $50,000 accurately. It would seem that using accuracy as a metric for evaluating a particular model’s performace would be appropriate. Additionally, identifying someone that does not make more than $50,000 as someone who does would be detrimental to CharityML, since they are looking to find individuals willing to donate. Therefore, a model’s ability to precisely predict those that make more than $50,000 is more important than the model’s ability to recall those individuals. We can use F-beta score as a metric that considers both precision and recall:\n\\[ F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall} \\]\nIn particular, when \\(\\beta = 0.5\\), more emphasis is placed on precision. This is called the F\\(_{0.5}\\) score (or F-score for simplicity).\nLooking at the distribution of classes (those who make at most $50,000, and those who make more), it’s clear most individuals do not make more than $50,000. This can greatly affect accuracy, since we could simply say “this person does not make more than $50,000” and generally be right, without ever looking at the data! Making such a statement would be called naive, since we have not considered any information to substantiate the claim. It is always important to consider the naive prediction for your data, to help establish a benchmark for whether a model is performing well. That been said, using that prediction would be pointless: If we predicted all people made less than $50,000, CharityML would identify no one as donors.\n\nNote: Recap of accuracy, precision, recall\n** Accuracy ** measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n** Precision ** tells us what proportion of messages we classified as spam, actually were spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classificatio), in other words it is the ratio of\n[True Positives/(True Positives + False Positives)]\n** Recall(sensitivity)** tells us what proportion of messages that actually were spam were classified by us as spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of\n[True Positives/(True Positives + False Negatives)]\nFor classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren’t, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average(harmonic mean) of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score(we take the harmonic mean as we are dealing with ratios).\n\n\n\nQuestion 1 - Naive Predictor Performace\n\nIf we chose a model that always predicted an individual made more than $50,000, what would that model’s accuracy and F-score be on this dataset? You must use the code cell below and assign your results to 'accuracy' and 'fscore' to be used later.\n\n** Please note ** that the the purpose of generating a naive predictor is simply to show what a base model without any intelligence would look like. In the real world, ideally your base model would be either the results of a previous model or could be based on a research paper upon which you are looking to improve. When there is no benchmark model set, getting a result better than random choice is a place you could start from.\n** HINT: **\n\nWhen we have a model that always predicts ‘1’ (i.e. the individual makes more than 50k) then our model will have no True Negatives(TN) or False Negatives(FN) as we are not making any negative(‘0’ value) predictions. Therefore our Accuracy in this case becomes the same as our Precision(True Positives/(True Positives + False Positives)) as every prediction that we have made with value ‘1’ that should have ‘0’ becomes a False Positive; therefore our denominator in this case is the total number of records we have in total.\nOur Recall score(True Positives/(True Positives + False Negatives)) in this setting becomes 1 as we have no False Negatives.\n\n\n'''\nTP = np.sum(income) # Counting the ones as this is the naive case. Note that 'income' is the 'income_raw' data \nencoded to numerical values done in the data preprocessing step.\nFP = income.count() - TP # Specific to the naive case\n\nTN = 0 # No predicted negatives in the naive case\nFN = 0 # No predicted negatives in the naive case\n'''\n# TODO: Calculate accuracy, precision and recall\nTP = np.sum(data[\"income\"] == \">50K\")\nFP = data[\"income\"].count() - TP\nTN = 0\nFN = 0\n\n\naccuracy = (data[\"income\"] == \">50K\").mean()\nrecall = TP / (TP + FN)\nprecision = TP / (TP + FP)\nbeta = 0.5\n# TODO: Calculate F-score using the formula above for beta = 0.5 and correct values for precision and recall.\nfscore = (1 + beta**2) * ((precision * recall) / ((beta**2 * precision) + recall))\n\n# Print the results \nprint(\"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))\n\nNaive Predictor: [Accuracy score: 0.2478, F-score: 0.2917]\n\n\n\n\nSupervised Learning Models\nThe following are some of the supervised learning models that are currently available in scikit-learn that you may choose from: - Gaussian Naive Bayes (GaussianNB) - Decision Trees - Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting) - K-Nearest Neighbors (KNeighbors) - Stochastic Gradient Descent Classifier (SGDC) - Support Vector Machines (SVM) - Logistic Regression\n\n\nQuestion 2 - Model Application\nList three of the supervised learning models above that are appropriate for this problem that you will test on the census data. For each model chosen\n\nDescribe one real-world application in industry where the model can be applied.\nWhat are the strengths of the model; when does it perform well?\nWhat are the weaknesses of the model; when does it perform poorly?\nWhat makes this model a good candidate for the problem, given what you know about the data?\n\n** HINT: **\nStructure your answer in the same format as above^, with 4 parts for each of the three models you pick. Please include references with your answer.\n\n\nSupport Vector Machines\n\nOne real-world application for SVMs is in the classification of cancer genomes (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5822181/). According to the linked article, SVMs are useful for finding subtypes of cancer genomes which are useful for diagnosis and treatment.\nOne strength of SVMs is the use of kernels to take data that is not easily separable for classification and tranform it into a higher dimension to separate the classes and classify the data. One example of this is using the tranform equation \\(x^2 + y^2\\) to tranform data into a third dimension and use a plane to separate the classes. RBF kernels are also useful when a single boundary would not be sufficient to classify the points, such as a case where there are multiple groupings of a class that may need to be separated out by a few circular boundaries.\nOne weakness of support vector machines is that the computation time tends to be larger than the othe methods chosen. SVMs also do not calculate the probability of being a specific class given a predictor variable and cannot be used for inference as can be done with logistic regression.\nThis model is a good candidate because it is a classification model that can handle non-linear class boundaries. For example, there may be multiple clusters in the data where people are likely to make more than 50K per year. Using a SVM and RBF kernels, we can bring those grouping into a higher dimension to separate them out even in the case where there is not a single boundary line that we can draw to sepearate them\n\n\n\nDecision Trees\n\nDecision trees can be useful in the medical industry for things like diagnosis because in addition to classifying, it is possible to see the hiearchy of decisions that the model used to make the classification which provides insights into which features are most important when determining whether someone has a particular illness.\nDecision trees recursively split data in the direction of the largest separability (information gain) to construct a decision tree using the predictor features to hone in on the correct classification. One advantage of this approach is it allows for the user to gain a better understand the hierarchy of decision making that was used to classify a point and understand which features are most important in the decision making.\nA weakness of decision trees are that they have a tendency to overfit the data. If a decision trees can create a very large hierarchical decision structure with more leaves than necessary which may lean towards memorizing the data.\nThis model is a good candidate for this analysis because we have many features that can be used to help determine whether someone makes more than 50K per year and we can also use the decision tree structure to get a better understanding of the important factors that are used in determining that classification.\n\n\n\nRandom Forests\n\nRandom Forests are applicable to many areas where a more generalized model is being sought and you are trying to avoid running into an issue of overfitting the data.\nRandom forests help solve the problem of overfitting by decisions trees by taking a random subset of features to construct the decision tree, and repeating the process using other random subsets of features. Then, when trying to classify the point, the class that was chosen most often by all the decision trees will be selected.\nOne weakness of Random Forests is despite helping reduce the variance of decisions trees, they can still be prone to overfitting\nThis model is a good candidate for this analysis because it will help to better generalize our classifier so we can continue to use this model for determining which potential donors make more than 50K a year.\n\n\n\nImplementation - Creating a Training and Predicting Pipeline\nTo properly evaluate the performance of each model you’ve chosen, it’s important that you create a training and predicting pipeline that allows you to quickly and effectively train models using various sizes of training data and perform predictions on the testing data. Your implementation here will be used in the following section. In the code block below, you will need to implement the following: - Import fbeta_score and accuracy_score from sklearn.metrics. - Fit the learner to the sampled training data and record the training time. - Perform predictions on the test data X_test, and also on the first 300 training points X_train[:300]. - Record the total prediction time. - Calculate the accuracy score for both the training subset and testing set. - Calculate the F-score for both the training subset and testing set. - Make sure that you set the beta parameter!\n\nX_train[:10]\n\n\n\n\n  \n    \n      \n      age\n      education-num\n      capital-gain\n      capital-loss\n      hours-per-week\n      workclass_ Federal-gov\n      workclass_ Local-gov\n      workclass_ Private\n      workclass_ Self-emp-inc\n      workclass_ Self-emp-not-inc\n      ...\n      native-country_ Portugal\n      native-country_ Puerto-Rico\n      native-country_ Scotland\n      native-country_ South\n      native-country_ Taiwan\n      native-country_ Thailand\n      native-country_ Trinadad&Tobago\n      native-country_ United-States\n      native-country_ Vietnam\n      native-country_ Yugoslavia\n    \n  \n  \n    \n      13181\n      0.410959\n      0.600000\n      0.0\n      0.000000\n      0.500000\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      10342\n      0.438356\n      0.533333\n      0.0\n      0.000000\n      0.397959\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      20881\n      0.054795\n      0.666667\n      0.0\n      0.000000\n      0.357143\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      24972\n      0.301370\n      0.866667\n      0.0\n      0.905759\n      0.448980\n      0\n      1\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      43867\n      0.246575\n      0.600000\n      0.0\n      0.000000\n      0.500000\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      4124\n      0.315068\n      0.533333\n      0.0\n      0.000000\n      0.397959\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      17641\n      0.054795\n      0.600000\n      0.0\n      0.000000\n      0.397959\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      17273\n      0.561644\n      0.400000\n      0.0\n      0.000000\n      0.397959\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      41191\n      0.109589\n      0.800000\n      0.0\n      0.000000\n      0.397959\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      5386\n      0.232877\n      0.800000\n      0.0\n      0.000000\n      0.377551\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n  \n\n10 rows × 103 columns\n\n\n\n\n# TODO: Import two metrics from sklearn - fbeta_score and accuracy_score\nfrom sklearn.metrics import fbeta_score, accuracy_score\n\ndef train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - sample_size: the size of samples (number) to be drawn from training set\n       - X_train: features training set\n       - y_train: income training set\n       - X_test: features testing set\n       - y_test: income testing set\n    '''\n    \n    results = {}\n    beta = 0.5\n    \n    # TODO: Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n    start = time() # Get start time\n    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time() # Get end time\n    \n    # TODO: Calculate the training time\n    results['train_time'] = end - start\n        \n    # TODO: Get the predictions on the test set(X_test),\n    #       then get predictions on the first 300 training samples(X_train) using .predict()\n    start = time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train.iloc[:300,:])\n    end = time() # Get end time\n    \n    # TODO: Calculate the total prediction time\n    results['pred_time'] = end - start\n            \n    # TODO: Compute accuracy on the first 300 training samples which is y_train[:300]\n    results['acc_train'] = accuracy_score(predictions_train, y_train[:300])\n        \n    # TODO: Compute accuracy on test set using accuracy_score()\n    results['acc_test'] = accuracy_score(predictions_test, y_test)\n    \n    # TODO: Compute F-score on the the first 300 training samples using fbeta_score()\n    results['f_train'] = fbeta_score(predictions_train, y_train[:300], beta = beta)\n        \n    # TODO: Compute F-score on the test set which is y_test\n    results['f_test'] =fbeta_score(predictions_test, y_test, beta = beta)\n       \n    # Success\n    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n        \n    # Return the results\n    return results\n\n\n\nImplementation: Initial Model Evaluation\nIn the code cell, you will need to implement the following: - Import the three supervised learning models you’ve discussed in the previous section. - Initialize the three models and store them in 'clf_A', 'clf_B', and 'clf_C'. - Use a 'random_state' for each model you use, if provided. - Note: Use the default settings for each model — you will tune one specific model in a later section. - Calculate the number of records equal to 1%, 10%, and 100% of the training data. - Store those values in 'samples_1', 'samples_10', and 'samples_100' respectively.\nNote: Depending on which algorithms you chose, the following implementation may take some time to run!\n\n# TODO: Import the three supervised learning models from sklearn\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# TODO: Initialize the three models\nclf_A = SVC(random_state=2)\nclf_B = DecisionTreeClassifier(random_state=2)\nclf_C = RandomForestClassifier(random_state=2)\n\n# TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data\n# HINT: samples_100 is the entire training set i.e. len(y_train)\n# HINT: samples_10 is 10% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\n# HINT: samples_1 is 1% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\nsamples_100 = X_train.shape[0]\nsamples_10 = int(X_train.shape[0] * .10)\nsamples_1 = int(X_train.shape[0] * .01)\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_A, clf_B, clf_C]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        results[clf_name][i] = \\\n        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n\n# Run metrics visualization for the three supervised learning models chosen\nvs.evaluate(results, accuracy, fscore)\n\nSVC trained on 361 samples.\n\n\nSVC trained on 3617 samples.\n\n\nSVC trained on 36177 samples.\nDecisionTreeClassifier trained on 361 samples.\nDecisionTreeClassifier trained on 3617 samples.\n\n\nDecisionTreeClassifier trained on 36177 samples.\nRandomForestClassifier trained on 361 samples.\n\n\nRandomForestClassifier trained on 3617 samples.\n\n\nRandomForestClassifier trained on 36177 samples.\n\n\n/Users/seanhoyt/Documents/Documents - Sean’s MacBook Pro/repos/website/posts/finding-donars-udacity/visuals.py:121: UserWarning: Tight layout not applied. tight_layout cannot make axes width small enough to accommodate all axes decorations\n  pl.tight_layout()"
  },
  {
    "objectID": "posts/finding-donars-udacity/index.html#improving-results",
    "href": "posts/finding-donars-udacity/index.html#improving-results",
    "title": "Finding Donors for CharityML",
    "section": "Improving Results",
    "text": "Improving Results\nIn this final section, you will choose from the three supervised learning models the best model to use on the student data. You will then perform a grid search optimization for the model over the entire training set (X_train and y_train) by tuning at least one parameter to improve upon the untuned model’s F-score.\n\nQuestion 3 - Choosing the Best Model\n\nBased on the evaluation you performed earlier, in one to two paragraphs, explain to CharityML which of the three models you believe to be most appropriate for the task of identifying individuals that make more than $50,000.\n\n** HINT: ** Look at the graph at the bottom left from the cell above(the visualization created by vs.evaluate(results, accuracy, fscore)) and check the F score for the testing set when 100% of the training set is used. Which model has the highest score? Your answer should include discussion of the: * metrics - F score on the testing when 100% of the training data is used, * prediction/training time * the algorithm’s suitability for the data.\nThe Random Forest model obtained the highest F score on the testing data when 100% of the training data was used. Random Forest performed slightly better than the decision tree model in all cases, as expected since it uses the decision tree as a weak learner, and the SVM in all cases. It does appear that the Random Forest model still has a high variance. A better bias variance tradeoff may be found using a grid search on the hyperparamters.\n\n\nQuestion 4 - Describing the Model in Layman’s Terms\n\nIn one to two paragraphs, explain to CharityML, in layman’s terms, how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical jargon, such as describing equations.\n\n** HINT: **\nWhen explaining your model, if using external resources please include all citations.\nThe Random Forest classifier is an ensemble method that creates multiple decision trees to help create a more generalized model. Decisions trees split the data along one feature to give the highest separability between the classes and continues this process recursively until a classification is found. One of the advantages of decision trees is that in addition to predictions, additional insights can be gained by understanding the hierachy of decisions that were chosen which shows which features were most important when making the prediction. One weakness of decsions trees is that they have a tendency to overfit the data.\nThe Random Forest classifier adds an additional layer onto the decision tree to mitigate issues with overfitting by taking a random subset of features from the training data and constructing a decision tree only using that subset of features. This process is then repeated many times with different random subsets of features from the training data. Then, when trying to classify the point, the class that was chosen most often by all the decision trees will be selected.\n\n\nImplementation: Model Tuning\nFine tune the chosen model. Use grid search (GridSearchCV) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following: - Import sklearn.grid_search.GridSearchCV and sklearn.metrics.make_scorer. - Initialize the classifier you’ve chosen and store it in clf. - Set a random_state if one is available to the same state you set before. - Create a dictionary of parameters you wish to tune for the chosen model. - Example: parameters = {'parameter' : [list of values]}. - Note: Avoid tuning the max_features parameter of your learner if that parameter is available! - Use make_scorer to create an fbeta_score scoring object (with \\(\\beta = 0.5\\)). - Perform grid search on the classifier clf using the 'scorer', and store it in grid_obj. - Fit the grid search object to the training data (X_train, y_train), and store it in grid_fit.\nNote: Depending on the algorithm chosen and the parameter list, the following implementation may take some time to run!\n\n# TODO: Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\n# TODO: Initialize the classifier\nclf = RandomForestClassifier(random_state = 2)\n\n# TODO: Create the parameters list you wish to tune, using a dictionary if needed.\n# HINT: parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}\nparameters = {\n            \"max_depth\": [10, None] ,\n            \"min_samples_split\": [1, 100],\n            \"min_samples_leaf\": [1, 100]}\n\n# TODO: Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(fbeta_score, beta=.5)\n\n# TODO: Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, parameters, scoring=scorer)\n\n# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\n\n/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n20 fits failed out of a total of 40.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n20 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n    trees = Parallel(\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n    return self.function(*args, **kwargs)\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 937, in fit\n    super().fit(\n  File \"/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 250, in fit\n    raise ValueError(\nValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/Users/seanhoyt/opt/anaconda3/envs/py3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.72748942        nan 0.70188377        nan 0.73756208\n        nan 0.70626374]\n  warnings.warn(\n\n\nUnoptimized model\n------\nAccuracy score on testing data: 0.8412\nF-score on testing data: 0.6787\n\nOptimized Model\n------\nFinal accuracy score on the testing data: 0.8609\nFinal F-score on the testing data: 0.7310\n\n\n\n\nQuestion 5 - Final Model Evaluation\n\nWhat is your optimized model’s accuracy and F-score on the testing data?\nAre these scores better or worse than the unoptimized model?\nHow do the results from your optimized model compare to the naive predictor benchmarks you found earlier in Question 1?_\n\nNote: Fill in the table below with your results, and then provide discussion in the Answer box.\n\nResults:\n\n\n\nMetric\nUnoptimized Model\nOptimized Model\n\n\n\n\nAccuracy Score\n0.8412\n0.8609\n\n\nF-score\n0.6787\n0.7310\n\n\n\nThe optimized model achieved an accuracy of 0.86 and F-score of 0.73. The accuracy improve by only 2% compared to the unoptimized model, but the F-score imporved. y 5% compared to the unoptimized model. Compared to the naive predictor, the accuracy increated by almost 10% and the F-score changed form 0 to 0.73."
  },
  {
    "objectID": "posts/finding-donars-udacity/index.html#feature-importance",
    "href": "posts/finding-donars-udacity/index.html#feature-importance",
    "title": "Finding Donors for CharityML",
    "section": "Feature Importance",
    "text": "Feature Importance\nAn important task when performing supervised learning on a dataset like the census data we study here is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do. In the case of this project, that means we wish to identify a small number of features that most strongly predict whether an individual makes at most or more than $50,000.\nChoose a scikit-learn classifier (e.g., adaboost, random forests) that has a feature_importance_ attribute, which is a function that ranks the importance of features according to the chosen classifier. In the next python cell fit this classifier to training set and use this attribute to determine the top 5 most important features for the census dataset.\n\nQuestion 6 - Feature Relevance Observation\nWhen Exploring the Data, it was shown there are thirteen available features for each individual on record in the census data. Of these thirteen records, which five features do you believe to be most important for prediction, and in what order would you rank them and why?\nGiven the thirteen features, I believe the five most important ranked in order will be: * age: People tend to make more money as they get older, making them more likely to make over 50K. * education: People with higher levels of education tend to make more money . * capital gain: The larget the capital gain returns, the more money a person is likely making since you need some excess money to invest in the first place. * marital-status: Two income households will have higher incomes. * race: systemic wealth inequality among races in the US make, for example, white americans to have a higher income on average than black ammericans.\n\n\nImplementation - Extracting Feature Importance\nChoose a scikit-learn supervised learning algorithm that has a feature_importance_ attribute availble for it. This attribute is a function that ranks the importance of each feature when making predictions based on the chosen algorithm.\nIn the code cell below, you will need to implement the following: - Import a supervised learning model from sklearn if it is different from the three used earlier. - Train the supervised model on the entire training set. - Extract the feature importances using '.feature_importances_'.\n\n# TODO: Import a supervised learning model that has 'feature_importances_'\n\n\n# TODO: Train the supervised model on the training set using .fit(X_train, y_train)\nmodel = RandomForestClassifier(random_state = 2).fit(X_train, y_train)\n\n# TODO: Extract the feature importances using .feature_importances_ \nimportances = model.feature_importances_\n\n# Plot\nvs.feature_plot(importances, X_train, y_train)\n\n\n\n\n\n\nQuestion 7 - Extracting Feature Importance\nObserve the visualization created above which displays the five most relevant features for predicting if an individual makes at most or above $50,000.\n* How do these five features compare to the five features you discussed in Question 6? * If you were close to the same answer, how does this visualization confirm your thoughts? * If you were not close, why do you think these features are more relevant?\nI correctly guessed 3 of the 5 features with all 3 being at the correct rank. I am not sure what ‘education-num’ means in the data so I did not select that on; I instead chose ‘education’, which I ranked in the #2 spot. I did not select hours per week because I thought that observations with long hours could be a mix of people multiple low paying jobs and make less than 50K or work high pressure, high paying jobs and make more than 50K which would make that difficult to use for classification.\n\n\nFeature Selection\nHow does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower — at the cost of performance metrics. From the visualization above, we see that the top five most important features contribute more than half of the importance of all features present in the data. This hints that we can attempt to reduce the feature space and simplify the information required for the model to learn. The code cell below will use the same optimized model you found earlier, and train it on the same training set with only the top five important features.\n\n# Import functionality for cloning a model\nfrom sklearn.base import clone\n\n# Reduce the feature space\nX_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]\nX_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]]\n\n# Train on the \"best\" model found from grid search earlier\nclf = (clone(best_clf)).fit(X_train_reduced, y_train)\n\n# Make new predictions\nreduced_predictions = clf.predict(X_test_reduced)\n\n# Report scores from the final model using both versions of data\nprint(\"Final Model trained on full data\\n------\")\nprint(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\nprint(\"\\nFinal Model trained on reduced data\\n------\")\nprint(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, reduced_predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, reduced_predictions, beta = 0.5)))\n\nFinal Model trained on full data\n------\nAccuracy on testing data: 0.8609\nF-score on testing data: 0.7310\n\nFinal Model trained on reduced data\n------\nAccuracy on testing data: 0.8471\nF-score on testing data: 0.6980\n\n\n\n\nQuestion 8 - Effects of Feature Selection\n\nHow does the final model’s F-score and accuracy score on the reduced data using only five features compare to those same scores when all features are used?\nIf training time was a factor, would you consider using the reduced data as your training set?\n\nThe metrics form the reduced data had a somewhat significant decrease and are closer to the values found from the unoptimized models. Because the Random Forest model did not take large amounts of computing time, I would likely stick with the full data or add a few more features to the reduced data to increase the F-score."
  },
  {
    "objectID": "posts/data-wrangling-udacity/index.html",
    "href": "posts/data-wrangling-udacity/index.html",
    "title": "Cleaning WeRateDogs Twitter Data",
    "section": "",
    "text": "Link to Github repository"
  },
  {
    "objectID": "posts/data-wrangling-udacity/index.html#data-gathering",
    "href": "posts/data-wrangling-udacity/index.html#data-gathering",
    "title": "Cleaning WeRateDogs Twitter Data",
    "section": "Data Gathering",
    "text": "Data Gathering\nIn the cell below, gather all three pieces of data for this project and load them in the notebook. Note: the methods required to gather each data are different. 1. Directly download the WeRateDogs Twitter archive data (twitter_archive_enhanced.csv)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport requests\nimport json\n\n\ntwitter_archive = pd.read_csv(\"twitter-archive-enhanced.csv\")\n\n\nUse the Requests library to download the tweet image prediction (image_predictions.tsv)\n\n\nimag_pred = requests.get(\"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\")\nwith open(\"image_predictions.tsv\", \"wb\") as f:\n    f.write(imag_pred.content)\n\n\nimage_predictions = pd.read_csv(\"image_predictions.tsv\", sep=\"\\t\")\n\n\nUse the Tweepy library to query additional data via the Twitter API (tweet_json.txt)\n\nThe code for the Twitter API is below, however it is commented out because I was not able to get the elevated privledges needed to use the API\n\n#import tweepy\n#from tweepy import OAuthHandler\n#import json\n#from timeit import default_timer as timer\n#\n# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n# These are hidden to comply with Twitter's API terms and conditions\n#consumer_key = 'HIDDEN'\n#consumer_secret = 'HIDDEN'\n#access_token = 'HIDDEN'\n#access_secret = 'HIDDEN'\n#\n#auth = OAuthHandler(consumer_key, consumer_secret)\n#auth.set_access_token(access_token, access_secret)\n#\n#api = tweepy.API(auth, wait_on_rate_limit=True)\n#\n# NOTE TO STUDENT WITH MOBILE VERIFICATION ISSUES:\n# df_1 is a DataFrame with the twitter_archive_enhanced.csv file. You may have to\n# change line 17 to match the name of your DataFrame with twitter_archive_enhanced.csv\n# NOTE TO REVIEWER: this student had mobile verification issues so the following\n# Twitter API code was sent to this student from a Udacity instructor\n# Tweet IDs for which to gather additional data via Twitter's API\n#tweet_ids = df_1.tweet_id.values\n#len(tweet_ids)\n\n# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n#count = 0\n#fails_dict = {}\n#start = timer()\n# Save each tweet's returned JSON as a new line in a .txt file\n#with open('tweet_json.txt', 'w') as outfile:\n#    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n#    for tweet_id in tweet_ids:\n#        count += 1\n#        print(str(count) + \": \" + str(tweet_id))\n#        try:\n#            tweet = api.get_status(tweet_id, tweet_mode='extended')\n#            print(\"Success\")\n#            json.dump(tweet._json, outfile)\n#            outfile.write('\\n')\n#        except tweepy.TweepError as e:\n#            print(\"Fail\")\n#            fails_dict[tweet_id] = e\n#            pass\n#end = timer()\n#print(end - start)\n#print(fails_dict)\n\n\ntweets = requests.get('https://video.udacity-data.com/topher/2018/November/5be5fb7d_tweet-json/tweet-json.txt')\nwith open('tweet_json.txt', 'wb') as f:\n        f.write(tweets.content)\n\n\ntweet_data = pd.read_json('tweet_json.txt', lines=True)"
  },
  {
    "objectID": "posts/data-wrangling-udacity/index.html#assessing-data",
    "href": "posts/data-wrangling-udacity/index.html#assessing-data",
    "title": "Cleaning WeRateDogs Twitter Data",
    "section": "Assessing Data",
    "text": "Assessing Data\nIn this section, detect and document at least eight (8) quality issues and two (2) tidiness issue. You must use both visual assessment programmatic assessement to assess the data.\nNote: pay attention to the following key points when you access the data.\n\nYou only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\nAssessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. Therefore, the requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.\nThe fact that the rating numerators are greater than the denominators does not need to be cleaned. This unique rating system is a big part of the popularity of WeRateDogs.\nYou do not need to gather the tweets beyond August 1st, 2017. You can, but note that you won’t be able to gather the image predictions for these tweets since you don’t have access to the algorithm used.\n\n\ntwitter_archive.head()\n\n\n\n\n  \n    \n      \n      tweet_id\n      in_reply_to_status_id\n      in_reply_to_user_id\n      timestamp\n      source\n      text\n      retweeted_status_id\n      retweeted_status_user_id\n      retweeted_status_timestamp\n      expanded_urls\n      rating_numerator\n      rating_denominator\n      name\n      doggo\n      floofer\n      pupper\n      puppo\n    \n  \n  \n    \n      0\n      892420643555336193\n      NaN\n      NaN\n      2017-08-01 16:23:56 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      This is Phineas. He's a mystical boy. Only eve...\n      NaN\n      NaN\n      NaN\n      https://twitter.com/dog_rates/status/892420643...\n      13\n      10\n      Phineas\n      None\n      None\n      None\n      None\n    \n    \n      1\n      892177421306343426\n      NaN\n      NaN\n      2017-08-01 00:17:27 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      This is Tilly. She's just checking pup on you....\n      NaN\n      NaN\n      NaN\n      https://twitter.com/dog_rates/status/892177421...\n      13\n      10\n      Tilly\n      None\n      None\n      None\n      None\n    \n    \n      2\n      891815181378084864\n      NaN\n      NaN\n      2017-07-31 00:18:03 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      This is Archie. He is a rare Norwegian Pouncin...\n      NaN\n      NaN\n      NaN\n      https://twitter.com/dog_rates/status/891815181...\n      12\n      10\n      Archie\n      None\n      None\n      None\n      None\n    \n    \n      3\n      891689557279858688\n      NaN\n      NaN\n      2017-07-30 15:58:51 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      This is Darla. She commenced a snooze mid meal...\n      NaN\n      NaN\n      NaN\n      https://twitter.com/dog_rates/status/891689557...\n      13\n      10\n      Darla\n      None\n      None\n      None\n      None\n    \n    \n      4\n      891327558926688256\n      NaN\n      NaN\n      2017-07-29 16:00:24 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      This is Franklin. He would like you to stop ca...\n      NaN\n      NaN\n      NaN\n      https://twitter.com/dog_rates/status/891327558...\n      12\n      10\n      Franklin\n      None\n      None\n      None\n      None\n    \n  \n\n\n\n\nLooks like I can use when retweeted_status_id is not null to determine which tweets are retweets. We do not want retweets in this dataset.\n\ntwitter_archive.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2356 entries, 0 to 2355\nData columns (total 17 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   tweet_id                    2356 non-null   int64  \n 1   in_reply_to_status_id       78 non-null     float64\n 2   in_reply_to_user_id         78 non-null     float64\n 3   timestamp                   2356 non-null   object \n 4   source                      2356 non-null   object \n 5   text                        2356 non-null   object \n 6   retweeted_status_id         181 non-null    float64\n 7   retweeted_status_user_id    181 non-null    float64\n 8   retweeted_status_timestamp  181 non-null    object \n 9   expanded_urls               2297 non-null   object \n 10  rating_numerator            2356 non-null   int64  \n 11  rating_denominator          2356 non-null   int64  \n 12  name                        2356 non-null   object \n 13  doggo                       2356 non-null   object \n 14  floofer                     2356 non-null   object \n 15  pupper                      2356 non-null   object \n 16  puppo                       2356 non-null   object \ndtypes: float64(4), int64(3), object(10)\nmemory usage: 313.0+ KB\n\n\n\ntwitter_archive[twitter_archive.retweeted_status_id.notnull()].head()\n\n\n\n\n  \n    \n      \n      tweet_id\n      in_reply_to_status_id\n      in_reply_to_user_id\n      timestamp\n      source\n      text\n      retweeted_status_id\n      retweeted_status_user_id\n      retweeted_status_timestamp\n      expanded_urls\n      rating_numerator\n      rating_denominator\n      name\n      doggo\n      floofer\n      pupper\n      puppo\n    \n  \n  \n    \n      19\n      888202515573088257\n      NaN\n      NaN\n      2017-07-21 01:02:36 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      RT @dog_rates: This is Canela. She attempted s...\n      8.874740e+17\n      4.196984e+09\n      2017-07-19 00:47:34 +0000\n      https://twitter.com/dog_rates/status/887473957...\n      13\n      10\n      Canela\n      None\n      None\n      None\n      None\n    \n    \n      32\n      886054160059072513\n      NaN\n      NaN\n      2017-07-15 02:45:48 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      RT @Athletics: 12/10 #BATP https://t.co/WxwJmv...\n      8.860537e+17\n      1.960740e+07\n      2017-07-15 02:44:07 +0000\n      https://twitter.com/dog_rates/status/886053434...\n      12\n      10\n      None\n      None\n      None\n      None\n      None\n    \n    \n      36\n      885311592912609280\n      NaN\n      NaN\n      2017-07-13 01:35:06 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      RT @dog_rates: This is Lilly. She just paralle...\n      8.305833e+17\n      4.196984e+09\n      2017-02-12 01:04:29 +0000\n      https://twitter.com/dog_rates/status/830583320...\n      13\n      10\n      Lilly\n      None\n      None\n      None\n      None\n    \n    \n      68\n      879130579576475649\n      NaN\n      NaN\n      2017-06-26 00:13:58 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      RT @dog_rates: This is Emmy. She was adopted t...\n      8.780576e+17\n      4.196984e+09\n      2017-06-23 01:10:23 +0000\n      https://twitter.com/dog_rates/status/878057613...\n      14\n      10\n      Emmy\n      None\n      None\n      None\n      None\n    \n    \n      73\n      878404777348136964\n      NaN\n      NaN\n      2017-06-24 00:09:53 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      RT @dog_rates: Meet Shadow. In an attempt to r...\n      8.782815e+17\n      4.196984e+09\n      2017-06-23 16:00:04 +0000\n      https://www.gofundme.com/3yd6y1c,https://twitt...\n      13\n      10\n      Shadow\n      None\n      None\n      None\n      None\n    \n  \n\n\n\n\nLooks like doggo, floofer, pupper, puppo are different “types” of dogs.\n\ntwitter_archive[twitter_archive.doggo != \"None\"].head()\n\n\n\n\n  \n    \n      \n      tweet_id\n      in_reply_to_status_id\n      in_reply_to_user_id\n      timestamp\n      source\n      text\n      retweeted_status_id\n      retweeted_status_user_id\n      retweeted_status_timestamp\n      expanded_urls\n      rating_numerator\n      rating_denominator\n      name\n      doggo\n      floofer\n      pupper\n      puppo\n    \n  \n  \n    \n      9\n      890240255349198849\n      NaN\n      NaN\n      2017-07-26 15:59:51 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      This is Cassie. She is a college pup. Studying...\n      NaN\n      NaN\n      NaN\n      https://twitter.com/dog_rates/status/890240255...\n      14\n      10\n      Cassie\n      doggo\n      None\n      None\n      None\n    \n    \n      43\n      884162670584377345\n      NaN\n      NaN\n      2017-07-09 21:29:42 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      Meet Yogi. He doesn't have any important dog m...\n      NaN\n      NaN\n      NaN\n      https://twitter.com/dog_rates/status/884162670...\n      12\n      10\n      Yogi\n      doggo\n      None\n      None\n      None\n    \n    \n      99\n      872967104147763200\n      NaN\n      NaN\n      2017-06-09 00:02:31 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      Here's a very large dog. He has a date later. ...\n      NaN\n      NaN\n      NaN\n      https://twitter.com/dog_rates/status/872967104...\n      12\n      10\n      None\n      doggo\n      None\n      None\n      None\n    \n    \n      108\n      871515927908634625\n      NaN\n      NaN\n      2017-06-04 23:56:03 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      This is Napolean. He's a Raggedy East Nicaragu...\n      NaN\n      NaN\n      NaN\n      https://twitter.com/dog_rates/status/871515927...\n      12\n      10\n      Napolean\n      doggo\n      None\n      None\n      None\n    \n    \n      110\n      871102520638267392\n      NaN\n      NaN\n      2017-06-03 20:33:19 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      Never doubt a doggo 14/10 https://t.co/AbBLh2FZCH\n      NaN\n      NaN\n      NaN\n      https://twitter.com/animalcog/status/871075758...\n      14\n      10\n      None\n      doggo\n      None\n      None\n      None\n    \n  \n\n\n\n\n\nimage_predictions.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2075 entries, 0 to 2074\nData columns (total 12 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   tweet_id  2075 non-null   int64  \n 1   jpg_url   2075 non-null   object \n 2   img_num   2075 non-null   int64  \n 3   p1        2075 non-null   object \n 4   p1_conf   2075 non-null   float64\n 5   p1_dog    2075 non-null   bool   \n 6   p2        2075 non-null   object \n 7   p2_conf   2075 non-null   float64\n 8   p2_dog    2075 non-null   bool   \n 9   p3        2075 non-null   object \n 10  p3_conf   2075 non-null   float64\n 11  p3_dog    2075 non-null   bool   \ndtypes: bool(3), float64(3), int64(2), object(4)\nmemory usage: 152.1+ KB\n\n\nLooks like the dog breeds and other obejcts appear multiple times. These could be considered categories\n\nimage_predictions.p3.value_counts()\n\nLabrador_retriever    79\nChihuahua             58\ngolden_retriever      48\nEskimo_dog            38\nkelpie                35\n                      ..\nox                     1\nassault_rifle          1\naxolotl                1\npot                    1\nbanana                 1\nName: p3, Length: 408, dtype: int64\n\n\n\nimage_predictions[image_predictions['p2_dog'] == False]\n\n\n\n\n  \n    \n      \n      tweet_id\n      jpg_url\n      img_num\n      p1\n      p1_conf\n      p1_dog\n      p2\n      p2_conf\n      p2_dog\n      p3\n      p3_conf\n      p3_dog\n    \n  \n  \n    \n      6\n      666051853826850816\n      https://pbs.twimg.com/media/CT5KoJ1WoAAJash.jpg\n      1\n      box_turtle\n      0.933012\n      False\n      mud_turtle\n      0.045885\n      False\n      terrapin\n      0.017885\n      False\n    \n    \n      8\n      666057090499244032\n      https://pbs.twimg.com/media/CT5PY90WoAAQGLo.jpg\n      1\n      shopping_cart\n      0.962465\n      False\n      shopping_basket\n      0.014594\n      False\n      golden_retriever\n      0.007959\n      True\n    \n    \n      17\n      666104133288665088\n      https://pbs.twimg.com/media/CT56LSZWoAAlJj2.jpg\n      1\n      hen\n      0.965932\n      False\n      cock\n      0.033919\n      False\n      partridge\n      0.000052\n      False\n    \n    \n      18\n      666268910803644416\n      https://pbs.twimg.com/media/CT8QCd1WEAADXws.jpg\n      1\n      desktop_computer\n      0.086502\n      False\n      desk\n      0.085547\n      False\n      bookcase\n      0.079480\n      False\n    \n    \n      21\n      666293911632134144\n      https://pbs.twimg.com/media/CT8mx7KW4AEQu8N.jpg\n      1\n      three-toed_sloth\n      0.914671\n      False\n      otter\n      0.015250\n      False\n      great_grey_owl\n      0.013207\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2044\n      886258384151887873\n      https://pbs.twimg.com/media/DEyfTG4UMAE4aE9.jpg\n      1\n      pug\n      0.943575\n      True\n      shower_cap\n      0.025286\n      False\n      Siamese_cat\n      0.002849\n      False\n    \n    \n      2046\n      886680336477933568\n      https://pbs.twimg.com/media/DE4fEDzWAAAyHMM.jpg\n      1\n      convertible\n      0.738995\n      False\n      sports_car\n      0.139952\n      False\n      car_wheel\n      0.044173\n      False\n    \n    \n      2050\n      887343217045368832\n      https://pbs.twimg.com/ext_tw_video_thumb/88734...\n      1\n      Mexican_hairless\n      0.330741\n      True\n      sea_lion\n      0.275645\n      False\n      Weimaraner\n      0.134203\n      True\n    \n    \n      2052\n      887517139158093824\n      https://pbs.twimg.com/ext_tw_video_thumb/88751...\n      1\n      limousine\n      0.130432\n      False\n      tow_truck\n      0.029175\n      False\n      shopping_cart\n      0.026321\n      False\n    \n    \n      2074\n      892420643555336193\n      https://pbs.twimg.com/media/DGKD1-bXoAAIAUK.jpg\n      1\n      orange\n      0.097049\n      False\n      bagel\n      0.085851\n      False\n      banana\n      0.076110\n      False\n    \n  \n\n522 rows × 12 columns\n\n\n\n\ntweet_data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2354 entries, 0 to 2353\nData columns (total 31 columns):\n #   Column                         Non-Null Count  Dtype              \n---  ------                         --------------  -----              \n 0   created_at                     2354 non-null   datetime64[ns, UTC]\n 1   id                             2354 non-null   int64              \n 2   id_str                         2354 non-null   int64              \n 3   full_text                      2354 non-null   object             \n 4   truncated                      2354 non-null   bool               \n 5   display_text_range             2354 non-null   object             \n 6   entities                       2354 non-null   object             \n 7   extended_entities              2073 non-null   object             \n 8   source                         2354 non-null   object             \n 9   in_reply_to_status_id          78 non-null     float64            \n 10  in_reply_to_status_id_str      78 non-null     float64            \n 11  in_reply_to_user_id            78 non-null     float64            \n 12  in_reply_to_user_id_str        78 non-null     float64            \n 13  in_reply_to_screen_name        78 non-null     object             \n 14  user                           2354 non-null   object             \n 15  geo                            0 non-null      float64            \n 16  coordinates                    0 non-null      float64            \n 17  place                          1 non-null      object             \n 18  contributors                   0 non-null      float64            \n 19  is_quote_status                2354 non-null   bool               \n 20  retweet_count                  2354 non-null   int64              \n 21  favorite_count                 2354 non-null   int64              \n 22  favorited                      2354 non-null   bool               \n 23  retweeted                      2354 non-null   bool               \n 24  possibly_sensitive             2211 non-null   float64            \n 25  possibly_sensitive_appealable  2211 non-null   float64            \n 26  lang                           2354 non-null   object             \n 27  retweeted_status               179 non-null    object             \n 28  quoted_status_id               29 non-null     float64            \n 29  quoted_status_id_str           29 non-null     float64            \n 30  quoted_status                  28 non-null     object             \ndtypes: bool(4), datetime64[ns, UTC](1), float64(11), int64(4), object(11)\nmemory usage: 505.9+ KB\n\n\n\ntwitter_archive.name\n\n0        Phineas\n1          Tilly\n2         Archie\n3          Darla\n4       Franklin\n          ...   \n2351        None\n2352           a\n2353           a\n2354           a\n2355        None\nName: name, Length: 2356, dtype: object\n\n\n\ntweet_data.id\n\n0       892420643555336193\n1       892177421306343426\n2       891815181378084864\n3       891689557279858688\n4       891327558926688256\n               ...        \n2349    666049248165822465\n2350    666044226329800704\n2351    666033412701032449\n2352    666029285002620928\n2353    666020888022790149\nName: id, Length: 2354, dtype: int64\n\n\n\nQuality issues\n\nThere are retweets in the dataset\ntwitter archive timestamp is a string not a time\nimage prediction p1, p2, p3 are strings, not categories\nall tweet ids are ints, but is not useful mathematically\nthe twitter archive source column has html in it.\nThe source column seems categorical, but is a string.\nSome dog beeds in the image predictions are lower case and some are upper case\nSome of the dog names in tweet_data are not names (“None”, “the”, “a”)\n\n\n\nTidiness issues\n\ndoggo, floofer pupper, puppo are columns rather than values in a single column.\nThe tweet archive and tweet data are separate tables"
  },
  {
    "objectID": "posts/data-wrangling-udacity/index.html#cleaning-data",
    "href": "posts/data-wrangling-udacity/index.html#cleaning-data",
    "title": "Cleaning WeRateDogs Twitter Data",
    "section": "Cleaning Data",
    "text": "Cleaning Data\nIn this section, clean all of the issues you documented while assessing.\nNote: Make a copy of the original data before cleaning. Cleaning includes merging individual pieces of data according to the rules of tidy data. The result should be a high-quality and tidy master pandas DataFrame (or DataFrames, if appropriate).\n\n# Make copies of original pieces of data\ntwitter_archive_clean = twitter_archive.copy()\ntweet_data_clean = tweet_data.copy()\nimage_predictions_clean = image_predictions.copy()\n\n\nIssue #1:\n\nThe tweet id and id are integers, but they are not useful mathematically in any way. Because these are references/identifiers for the tweets, they should be strings. We will change each tweet id to a string.\n\n\nCode\n\ntwitter_archive_clean.tweet_id = twitter_archive_clean.tweet_id.astype(str)\ntweet_data_clean.id = tweet_data_clean.id.astype(str)\nimage_predictions_clean.tweet_id = image_predictions_clean.tweet_id.astype(str)\n\n\n\nTest\n\nassert twitter_archive_clean.tweet_id.dtype == pd.Series(dtype=str).dtype\nassert tweet_data_clean.id.dtype == pd.Series(dtype=str).dtype\nassert image_predictions_clean.tweet_id.dtype == pd.Series(dtype=str).dtype\n\n\n\n\nIssue #2:\n\nThe tweet archive and tweet data are separate tables. These should be combined into one so we have one table containing all of the twitter data.\n\n\nCode\nChange the name of the id column in twee_data from id to tweet_id to help with the merge.\n\ntweet_data_clean.rename(columns={\"id\":\"tweet_id\"}, inplace=True)\n\n\ntweet_data_clean.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2354 entries, 0 to 2353\nData columns (total 31 columns):\n #   Column                         Non-Null Count  Dtype              \n---  ------                         --------------  -----              \n 0   created_at                     2354 non-null   datetime64[ns, UTC]\n 1   tweet_id                       2354 non-null   object             \n 2   id_str                         2354 non-null   int64              \n 3   full_text                      2354 non-null   object             \n 4   truncated                      2354 non-null   bool               \n 5   display_text_range             2354 non-null   object             \n 6   entities                       2354 non-null   object             \n 7   extended_entities              2073 non-null   object             \n 8   source                         2354 non-null   object             \n 9   in_reply_to_status_id          78 non-null     float64            \n 10  in_reply_to_status_id_str      78 non-null     float64            \n 11  in_reply_to_user_id            78 non-null     float64            \n 12  in_reply_to_user_id_str        78 non-null     float64            \n 13  in_reply_to_screen_name        78 non-null     object             \n 14  user                           2354 non-null   object             \n 15  geo                            0 non-null      float64            \n 16  coordinates                    0 non-null      float64            \n 17  place                          1 non-null      object             \n 18  contributors                   0 non-null      float64            \n 19  is_quote_status                2354 non-null   bool               \n 20  retweet_count                  2354 non-null   int64              \n 21  favorite_count                 2354 non-null   int64              \n 22  favorited                      2354 non-null   bool               \n 23  retweeted                      2354 non-null   bool               \n 24  possibly_sensitive             2211 non-null   float64            \n 25  possibly_sensitive_appealable  2211 non-null   float64            \n 26  lang                           2354 non-null   object             \n 27  retweeted_status               179 non-null    object             \n 28  quoted_status_id               29 non-null     float64            \n 29  quoted_status_id_str           29 non-null     float64            \n 30  quoted_status                  28 non-null     object             \ndtypes: bool(4), datetime64[ns, UTC](1), float64(11), int64(3), object(12)\nmemory usage: 505.9+ KB\n\n\n\ntwitter_archive_clean.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2356 entries, 0 to 2355\nData columns (total 17 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   tweet_id                    2356 non-null   object \n 1   in_reply_to_status_id       78 non-null     float64\n 2   in_reply_to_user_id         78 non-null     float64\n 3   timestamp                   2356 non-null   object \n 4   source                      2356 non-null   object \n 5   text                        2356 non-null   object \n 6   retweeted_status_id         181 non-null    float64\n 7   retweeted_status_user_id    181 non-null    float64\n 8   retweeted_status_timestamp  181 non-null    object \n 9   expanded_urls               2297 non-null   object \n 10  rating_numerator            2356 non-null   int64  \n 11  rating_denominator          2356 non-null   int64  \n 12  name                        2356 non-null   object \n 13  doggo                       2356 non-null   object \n 14  floofer                     2356 non-null   object \n 15  pupper                      2356 non-null   object \n 16  puppo                       2356 non-null   object \ndtypes: float64(4), int64(2), object(11)\nmemory usage: 313.0+ KB\n\n\n\nfull_tweet_data_clean = pd.merge(twitter_archive_clean, tweet_data_clean[[\"tweet_id\", \"retweet_count\", \"favorite_count\"]], on=\"tweet_id\", how=\"left\")\n\nThere are two tweets from the twitter_archive_clean that were not present in the tweet_data_clean, however, these tweets are retweets, so the will soon be removed and we can ignore them for now.\n\nfull_tweet_data_clean[full_tweet_data_clean.favorite_count.isnull()]\n\n\n\n\n  \n    \n      \n      tweet_id\n      in_reply_to_status_id\n      in_reply_to_user_id\n      timestamp\n      source\n      text\n      retweeted_status_id\n      retweeted_status_user_id\n      retweeted_status_timestamp\n      expanded_urls\n      rating_numerator\n      rating_denominator\n      name\n      doggo\n      floofer\n      pupper\n      puppo\n      retweet_count\n      favorite_count\n    \n  \n  \n    \n      19\n      888202515573088257\n      NaN\n      NaN\n      2017-07-21 01:02:36 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      RT @dog_rates: This is Canela. She attempted s...\n      8.874740e+17\n      4.196984e+09\n      2017-07-19 00:47:34 +0000\n      https://twitter.com/dog_rates/status/887473957...\n      13\n      10\n      Canela\n      None\n      None\n      None\n      None\n      NaN\n      NaN\n    \n    \n      815\n      771004394259247104\n      NaN\n      NaN\n      2016-08-31 15:19:06 +0000\n      <a href=\"http://twitter.com/download/iphone\" r...\n      RT @katieornah: @dog_rates learning a lot at c...\n      7.710021e+17\n      1.732729e+09\n      2016-08-31 15:10:07 +0000\n      https://twitter.com/katieornah/status/77100213...\n      12\n      10\n      None\n      None\n      None\n      pupper\n      None\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\nTest\nWe can see from .info() that the tables were merged.\n\nfull_tweet_data_clean.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2356 entries, 0 to 2355\nData columns (total 19 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   tweet_id                    2356 non-null   object \n 1   in_reply_to_status_id       78 non-null     float64\n 2   in_reply_to_user_id         78 non-null     float64\n 3   timestamp                   2356 non-null   object \n 4   source                      2356 non-null   object \n 5   text                        2356 non-null   object \n 6   retweeted_status_id         181 non-null    float64\n 7   retweeted_status_user_id    181 non-null    float64\n 8   retweeted_status_timestamp  181 non-null    object \n 9   expanded_urls               2297 non-null   object \n 10  rating_numerator            2356 non-null   int64  \n 11  rating_denominator          2356 non-null   int64  \n 12  name                        2356 non-null   object \n 13  doggo                       2356 non-null   object \n 14  floofer                     2356 non-null   object \n 15  pupper                      2356 non-null   object \n 16  puppo                       2356 non-null   object \n 17  retweet_count               2354 non-null   float64\n 18  favorite_count              2354 non-null   float64\ndtypes: float64(6), int64(2), object(11)\nmemory usage: 368.1+ KB\n\n\n\n\n\nIssue #3:\n\nThe retweets in the dataset should be removed because we only care about dog rating tweets.\n\n\nCode\nWe can use the retweeted_status_id column to help remove any retweets. If this column is not null, then it is a retweet.\n\nfull_tweet_data_clean = full_tweet_data_clean[full_tweet_data_clean.retweeted_status_id.isnull()]\n\n\n\nTest\ntest to ensure that every value in retweeted_status_id is null (meaning none are retweets)\n\nassert all(full_tweet_data_clean.retweeted_status_id.isnull())\n\n\n\n\nIssue #4:\n\nthe timestamp is a string, not a time. This should be converted to a time object\n\n\nCode\n\nfull_tweet_data_clean.timestamp = pd.to_datetime(full_tweet_data_clean.timestamp)\n\n\n\nTest\nWe can see from .info() that the timestamp column is now a datetime.\n\nfull_tweet_data_clean.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2175 entries, 0 to 2355\nData columns (total 19 columns):\n #   Column                      Non-Null Count  Dtype              \n---  ------                      --------------  -----              \n 0   tweet_id                    2175 non-null   object             \n 1   in_reply_to_status_id       78 non-null     float64            \n 2   in_reply_to_user_id         78 non-null     float64            \n 3   timestamp                   2175 non-null   datetime64[ns, UTC]\n 4   source                      2175 non-null   object             \n 5   text                        2175 non-null   object             \n 6   retweeted_status_id         0 non-null      float64            \n 7   retweeted_status_user_id    0 non-null      float64            \n 8   retweeted_status_timestamp  0 non-null      object             \n 9   expanded_urls               2117 non-null   object             \n 10  rating_numerator            2175 non-null   int64              \n 11  rating_denominator          2175 non-null   int64              \n 12  name                        2175 non-null   object             \n 13  doggo                       2175 non-null   object             \n 14  floofer                     2175 non-null   object             \n 15  pupper                      2175 non-null   object             \n 16  puppo                       2175 non-null   object             \n 17  retweet_count               2175 non-null   float64            \n 18  favorite_count              2175 non-null   float64            \ndtypes: datetime64[ns, UTC](1), float64(6), int64(2), object(10)\nmemory usage: 339.8+ KB\n\n\n\n\n\nIssue #5\n\nthe image predictions p1, p2, p3 should be converted from strings to categories\n\n\nCode\n\nimage_predictions_clean.p1 = image_predictions_clean.p1.astype('category')\nimage_predictions_clean.p2 = image_predictions_clean.p2.astype('category')\nimage_predictions_clean.p3 = image_predictions_clean.p3.astype('category')\n\n\n\nTest\nThe p1, p2, p3 columsn are now of type category\n\nimage_predictions_clean.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2075 entries, 0 to 2074\nData columns (total 12 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   tweet_id  2075 non-null   object  \n 1   jpg_url   2075 non-null   object  \n 2   img_num   2075 non-null   int64   \n 3   p1        2075 non-null   category\n 4   p1_conf   2075 non-null   float64 \n 5   p1_dog    2075 non-null   bool    \n 6   p2        2075 non-null   category\n 7   p2_conf   2075 non-null   float64 \n 8   p2_dog    2075 non-null   bool    \n 9   p3        2075 non-null   category\n 10  p3_conf   2075 non-null   float64 \n 11  p3_dog    2075 non-null   bool    \ndtypes: bool(3), category(3), float64(3), int64(1), object(2)\nmemory usage: 165.4+ KB\n\n\n\n\n\nIssue #6\n\nthe source column should have the html removed and the source converted to a cateogory\n\n\nCode\n\nfull_tweet_data_clean.source[0]\n\n'<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>'\n\n\nUsing a regular expression to get the text inside the html tag in the source column (regex source: https://stackoverflow.com/questions/33120584/python-regex-find-string-between-html-tags)\n\nfull_tweet_data_clean[\"source\"] = full_tweet_data_clean.source.str.extract(r'>(.+?)<')\n\n\n\nTest\nWe can get the value_counts for the source column to confirm that the sources were properly extracted from the html tag.\n\nfull_tweet_data_clean.source.value_counts()\n\nTwitter for iPhone     2042\nVine - Make a Scene      91\nTwitter Web Client       31\nTweetDeck                11\nName: source, dtype: int64\n\n\n\n\n\nIssue #7\n\nThe source column is a string, but should be converted to a category.\n\n\nCode\n\nfull_tweet_data_clean.source = full_tweet_data_clean.source.astype('category')\n\n\n\nTest\nWe can use .info() to confirm that the source column has been converted to a category\n\nfull_tweet_data_clean.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2175 entries, 0 to 2355\nData columns (total 19 columns):\n #   Column                      Non-Null Count  Dtype              \n---  ------                      --------------  -----              \n 0   tweet_id                    2175 non-null   object             \n 1   in_reply_to_status_id       78 non-null     float64            \n 2   in_reply_to_user_id         78 non-null     float64            \n 3   timestamp                   2175 non-null   datetime64[ns, UTC]\n 4   source                      2175 non-null   category           \n 5   text                        2175 non-null   object             \n 6   retweeted_status_id         0 non-null      float64            \n 7   retweeted_status_user_id    0 non-null      float64            \n 8   retweeted_status_timestamp  0 non-null      object             \n 9   expanded_urls               2117 non-null   object             \n 10  rating_numerator            2175 non-null   int64              \n 11  rating_denominator          2175 non-null   int64              \n 12  name                        2175 non-null   object             \n 13  doggo                       2175 non-null   object             \n 14  floofer                     2175 non-null   object             \n 15  pupper                      2175 non-null   object             \n 16  puppo                       2175 non-null   object             \n 17  retweet_count               2175 non-null   float64            \n 18  favorite_count              2175 non-null   float64            \ndtypes: category(1), datetime64[ns, UTC](1), float64(6), int64(2), object(9)\nmemory usage: 389.7+ KB\n\n\n\n\n\nIssue #8\n\nSome dog beeds in the image predictions are lower case and some are upper case. These should be changed to all lower case for consistency.\n\n\nCode\n\nimage_predictions_clean.p1 = image_predictions_clean.p1.str.lower()\nimage_predictions_clean.p2 = image_predictions_clean.p2.str.lower()\nimage_predictions_clean.p3 = image_predictions_clean.p3.str.lower()\n\n\n\nTest\n.islower() returns True if all characters are lower case and false if at least one is not lower case. Using this method, we can confirm that all were converted to lower.\n\nassert all(image_predictions_clean.p1.str.islower())\n\n\n\n\nIssue #9\n\nSome of the dog names in tweet_data are not names (“None”, “the”, “a”). These should all be made null to indicate that there is no name available.\n\n\nCode\n\nfull_tweet_data_clean.name = full_tweet_data_clean.name.replace([\"None\", \"a\", \"an\", \"the\", \"quite\"], np.nan)\n\n\n\nTest\nLooking at value counts and some samples, we can check to make sure these values are gone. This doesn’t necesarrily mean that all non-names are gone, but it looks like we removed the bulk of the issue and we can iterate and remove more later if more are found.\n\nfull_tweet_data_clean.name.value_counts()\n\nCharlie       11\nLucy          11\nOliver        10\nCooper        10\nTucker         9\n              ..\nWishes         1\nRose           1\nTheo           1\nFido           1\nChristoper     1\nName: name, Length: 951, dtype: int64\n\n\n\nfull_tweet_data_clean.name.sample(10)\n\n2055     Tanner\n743        Bear\n1942        NaN\n704       Rusty\n2102      Pluto\n120     Stanley\n2258        NaN\n638        Dave\n2007       Kreg\n1054       Bell\nName: name, dtype: object\n\n\n\n\n\nIssue #10\n\ndoggo, floofer pupper, puppo are columns rather than values in a single column. There should be a single column containing each type.\n\n\nCode\nFirst we’ll replace all the None values with empty strings to help with concatenating the column text\n\nfull_tweet_data_clean[\"doggo\"] = full_tweet_data_clean[\"doggo\"].replace(\"None\", \"\")\nfull_tweet_data_clean[\"floofer\"] = full_tweet_data_clean[\"floofer\"].replace(\"None\", \"\")\nfull_tweet_data_clean[\"pupper\"] = full_tweet_data_clean[\"pupper\"].replace(\"None\", \"\")\nfull_tweet_data_clean[\"puppo\"] = full_tweet_data_clean[\"puppo\"].replace(\"None\", \"\")\n\nThen we’ll concatentate all the columns into a new columns called dog_stage\n\nfull_tweet_data_clean[\"dog_stage\"] = full_tweet_data_clean.doggo.map(str) + full_tweet_data_clean.floofer.map(str) + full_tweet_data_clean.pupper.map(str) + full_tweet_data_clean.puppo.map(str)\n\nThen we’ll go back and replace the empty strings back to nans\n\nfull_tweet_data_clean[\"dog_stage\"] = full_tweet_data_clean[\"dog_stage\"].replace(\"\", np.nan)\n\nThen drop the old columns\n\nfull_tweet_data_clean.drop(columns=[\"doggo\", \"floofer\", \"pupper\", \"puppo\"], inplace=True)\n\nFrom the value counts we can see that there are a few instances of different stages in the same observations. We can make the convention that any conflicts containing the doggo stage will be resolved by using the doggo stage.\n\nfull_tweet_data_clean.dog_stage.value_counts()\n\npupper          224\ndoggo            75\npuppo            24\ndoggopupper      10\nfloofer           9\ndoggopuppo        1\ndoggofloofer      1\nName: dog_stage, dtype: int64\n\n\nUsing a regular expression, we can replace the values that start with doggo, to only use the doggo type.\n\nfull_tweet_data_clean.dog_stage = full_tweet_data_clean.dog_stage.replace(r'doggo\\w+', 'doggo', regex=True)\n\nWe can then convert this column to be of category type.\n\nfull_tweet_data_clean.dog_stage = full_tweet_data_clean.dog_stage.astype('category')\n\n\n\nTest\nWe can use .info() to check that the old columns were removed, the new column exists and it is a category type\n\nfull_tweet_data_clean.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2175 entries, 0 to 2355\nData columns (total 16 columns):\n #   Column                      Non-Null Count  Dtype              \n---  ------                      --------------  -----              \n 0   tweet_id                    2175 non-null   object             \n 1   in_reply_to_status_id       78 non-null     float64            \n 2   in_reply_to_user_id         78 non-null     float64            \n 3   timestamp                   2175 non-null   datetime64[ns, UTC]\n 4   source                      2175 non-null   category           \n 5   text                        2175 non-null   object             \n 6   retweeted_status_id         0 non-null      float64            \n 7   retweeted_status_user_id    0 non-null      float64            \n 8   retweeted_status_timestamp  0 non-null      object             \n 9   expanded_urls               2117 non-null   object             \n 10  rating_numerator            2175 non-null   int64              \n 11  rating_denominator          2175 non-null   int64              \n 12  name                        1423 non-null   object             \n 13  retweet_count               2175 non-null   float64            \n 14  favorite_count              2175 non-null   float64            \n 15  dog_stage                   344 non-null    category           \ndtypes: category(2), datetime64[ns, UTC](1), float64(6), int64(2), object(5)\nmemory usage: 324.1+ KB\n\n\nThen we can use value_counts to make sure the correct values are being used in the dog_stage column\n\nfull_tweet_data_clean.dog_stage.value_counts()\n\npupper     224\ndoggo       87\npuppo       24\nfloofer      9\nName: dog_stage, dtype: int64"
  },
  {
    "objectID": "posts/data-wrangling-udacity/index.html#storing-data",
    "href": "posts/data-wrangling-udacity/index.html#storing-data",
    "title": "Cleaning WeRateDogs Twitter Data",
    "section": "Storing Data",
    "text": "Storing Data\nSave gathered, assessed, and cleaned master dataset to a CSV file named “twitter_archive_master.csv”.\n\nfull_tweet_data_clean.to_csv(\"twitter_archive_master.csv\")\nimage_predictions_clean.to_csv(\"image_predictions_master.csv\")"
  },
  {
    "objectID": "posts/data-wrangling-udacity/index.html#analyzing-and-visualizing-data",
    "href": "posts/data-wrangling-udacity/index.html#analyzing-and-visualizing-data",
    "title": "Cleaning WeRateDogs Twitter Data",
    "section": "Analyzing and Visualizing Data",
    "text": "Analyzing and Visualizing Data\nIn this section, analyze and visualize your wrangled data. You must produce at least three (3) insights and one (1) visualization.\nThe vast majority of rating ratios were centered between 1 and 1.25. The plot below was cut off at 2 because there are ouliers that fall on the far positive side of the plot.\n\nplt.hist(full_tweet_data_clean.rating_numerator / full_tweet_data_clean.rating_denominator, bins=10, range=(0,2));\nplt.xlabel(\"Rating Ratio\");\nplt.ylabel(\"Tweets\");\nplt.title(\"Rating Ratio Distribution\");\n\n\n\n\nBased on the below plot, it appears that the favorite counts grew steadily over time, but also had an increase in variance along the y axis.\n\nfig = full_tweet_data_clean.plot(x=\"timestamp\", y=\"favorite_count\", title=\"Favorite Counts Over Time\");\nfig.set_xlabel(\"Date\")\nfig.set_ylabel(\"Favorites\")\n\nText(0, 0.5, 'Favorites')\n\n\n\n\n\nFrom the below plot, we can see that the vast majority of tweets were made from an iphone, and only a few were made from other sources such as TweetDeck, Twitter Web Client, and Vine.\n\nfig = full_tweet_data_clean.source.value_counts().plot.barh(title=\"Tweet Sources\");\nfig.set_xlabel(\"Tweets\")\nfig.set_ylabel(\"Tweet Source\")\n\nText(0, 0.5, 'Tweet Source')\n\n\n\n\n\n\nInsights:\n\nMost dog rating ratios were between 1 and 1.25 (numerator / denominator)\nOver time, the favorite counts grew for each tweet on average and the variance of number of favorites each tweet go also seems to have increased over time.\nThe vast majority of tweets were made on an iPhone, but a few other sources were also used: TweetDeck, Twitter Web Client, and Vine."
  },
  {
    "objectID": "posts/alt-fuel-station-analysis/index.html",
    "href": "posts/alt-fuel-station-analysis/index.html",
    "title": "Alternative Fuel Stations",
    "section": "",
    "text": "Link to Github repository"
  },
  {
    "objectID": "posts/alt-fuel-station-analysis/index.html#clean-data",
    "href": "posts/alt-fuel-station-analysis/index.html#clean-data",
    "title": "Alternative Fuel Stations",
    "section": "Clean Data",
    "text": "Clean Data\nOur main set of Census data is cleaned by removing any observations that aren’t states, reshaping the dataframe so each year is in a single column, joining with the state_ref dataframe to get the state land area and finally calculating the population densities. The other set of Census data we’ll use to get values for 2020 is filtered and joined to our main Census data.\n\n pop_density_2020 <- pop_raw_2020 |> \n    # only look at the year 2020 and state data\n    filter(Year == 2020, `Geography Type` == \"State\") |> \n    # rename and select relevant columns\n    transmute(\n      state = Name, \n      year = as.character(Year), \n      pop_den = `Resident Population Density`, \n      pop=`Resident Population`) |> \n    # join with the states_ref data to get the state abbreviations\n    inner_join(states_ref, by=c(\"state\" = \"name\")) |> \n    select(state, year, pop)\n\npop_density <- pop_raw |> \n  # remove non state observation\n  filter(STATE != \"00\") |> \n  # gather the pop estimate column headers into a single column for \n  # year and the values into a new pop column\n  pivot_longer(\n    cols = starts_with(\"POPESTIMATE\"),\n    names_to = \"year\", \n    values_to = \"pop\") |> \n  # downselect the columns\n  transmute(state = NAME, year, pop) |> \n  # separate the year from the POPESTIMATE text \n  separate(col=year, sep = \"POPESTIMATE\", into=c(NA, \"year\")) |> \n  # add 2020 data\n  bind_rows(pop_density_2020) |> \n  # join with the state area data\n  inner_join(states_ref, by=c(\"state\" = \"name\")) |> \n  # calcualte the population density\n  mutate(pop_den = pop / area, year = as.numeric(year)) |> \n  # select the columns of interest\n  select(state, abbr, year, pop_den, area)\n\nThe stations_raw data is cleaned by changing the categorical data to factors, changing the OPEN_DATE variable from a string to a date, and creating a new open_year variable that contains only the year that the station was opened. The latitude/longitudes are also filtered to restrict the data to only that found in the US. The data is then joined with the census data.\nSince much of this analysis refers to population data and we do not have population densities after the year 2020 available, we will drop any station data with the inner join for years where we do not have population density data (before 2010, after 2020).\n\nstations <- stations_raw |> \n    # rename columns, convert to factors and dates\n    transmute(\n      state = factor(STATE), \n      city = factor(CITY), \n      lat = LATITUDE, \n      lon = LONGITUDE, \n      fuel_type = factor(FUEL_TYPE_CODE),\n      open_date = ymd_hms(OPEN_DATE),\n      open_year = as.numeric(year(OPEN_DATE))\n      ) |> \n    # filter by lat/lon for continental US\n    filter(lat > 24 & lat < 52, lon < -64 & lon > -126) |> \n    # join with the population data\n    inner_join(pop_density, by=c(\"state\" = \"abbr\", \"open_year\" = \"year\")) |> \n    # rename the state columns\n    rename(abbr = state, state = state.y) |> \n    # move the state column to the front\n    relocate(state, .before = everything())"
  },
  {
    "objectID": "posts/alt-fuel-station-analysis/index.html#fuel-types",
    "href": "posts/alt-fuel-station-analysis/index.html#fuel-types",
    "title": "Alternative Fuel Stations",
    "section": "Fuel Types",
    "text": "Fuel Types\nWe can get a clearer picture of how many more electric fuel stations there are than any other kind with a direct comparison.\n\nstations |>\n    group_by(fuel_type) |> \n    summarize(n = n()) |> \n    mutate(fuel_type = fct_reorder(fuel_type, n)) |> \n    ggplot(aes(fuel_type, n)) + \n    geom_col(show.legend = FALSE, fill=\"darkblue\") + \n    coord_flip() +\n    labs(\n      x=\"\", \n      y=\"Total Stations\", \n      title=\"Total Number of Stations for Each Fuel Type\"\n      )\n\n\n\n\nIt’s clear from the plot above that electric fuel stations vastly outnumber every other fuel station combined. Because electric stations make up the vast majority of stations, we will reduce our scope by filtering our data to only contain electric fueling stations.\n\nstation_elec <- stations |> \n    filter(fuel_type == \"ELEC\")\n\nFocusing on electric fuel stations, we can look at which cities have the most stations.\n\nstation_elec |> \n    # count the stations in each city, state\n    count(state, city) |> \n    # order from most to least\n    arrange(desc(n)) |> \n    # get the top 20 cities with the most stations\n    head(20) |> \n    # reorder the factors based on the most stations\n    mutate(city = fct_reorder(city, n)) |> \n    ggplot(aes(city, n, fill=state)) +\n    geom_col() + \n    coord_flip() + \n    labs(\n      x=\"\", \n      y=\"Total Electric Stations\", \n      title=\"Total Electric Stations by City\", \n      fill=\"State\"\n      )\n\n\n\n\nThe city of Los Angeles has the most electric fuel stations and about twice as much as the San Diego which has the second highest number of stations. 9 out of the 20 cities with the highest number of electric fuel stations are in California, indicating that California may contain more stations than any other state.\nWe can confirm this by now plotting the states with the highest number of stations.\n\nstation_elec |> \n    # count the number of stations in each state\n    count(state) |>  \n    # reorder the factors based on number of stations\n    mutate(state = fct_reorder(state, n)) |> \n    ggplot(aes(state, n)) +\n    geom_col(show.legend = FALSE, fill=\"darkblue\") + \n    coord_flip() + \n    labs(\n      x=\"\", \n      y=\"Total Number of Stations\", \n      title=\"Total Number of Electric Stations by State\"\n      )\n\n\n\n\nBased on the plot above, California clearly leads the country in number of electric fueling stations. How do they compare when taking into account the size of the state?\nBy dividing the total number of stations each state has by the area of the state we can get a “station density” for each state to better compare stations across each state. For example, if a very large state such as Texas has as many fueling stations as a smaller state like Massachusetts, then Massachusetts likely has better electric station coverage than Texas as the stations per area is important for fueling during travel.\n\n# calculate station densities\nstation_den <- station_elec |> \n  # count the station stations for each state and year\n  count(state, open_year, pop_den, area, name=\"num_stations\") |> \n  # group by the state to prepare for a cumulative sum\n  group_by(state) |> \n  # arrange by state and year later sum in the correct order\n  arrange(state, open_year) |> \n  # calculate the cumulative sum of stations for eachs state\n  # and calcualte station densities over time\n  mutate(\n    tot_stations = cumsum(num_stations), \n    station_density = tot_stations / area\n    )  |> \n  # keep the following features\n  select(state, open_year, pop_den, station_density) |>\n  ungroup() |> \n  # make sure every state / year combination exists\n  complete(state, open_year) |>\n  # group by the state\n  group_by(state) |> \n  # fill in any missing state / year combinations with the \n  # densities from the previous year\n  fill(pop_den, station_density, .direction=\"down\") |> \n  ungroup() |> \n  # any NAs will be because there was no previous data, so\n  # these can be set to zero\n  mutate(pop_den = replace_na(pop_den, 0), station_density = replace_na(station_density, 0))\n\nWe can plot the newly calculated station densities:\n\nstation_den |> \n    filter(open_year == 2020) |> \n    mutate(state = fct_reorder(state, station_density)) |> \n    ggplot(aes(state, station_density)) +\n    geom_col(show.legend = FALSE, fill=\"darkblue\") + \n    coord_flip() +\n    labs(\n      x=\"\", \n      y=\"Electric Station Density (stations / sq. mi.)\", \n      title=\"Electric Station Density by State\"\n      )"
  },
  {
    "objectID": "posts/alt-fuel-station-analysis/index.html#conclusion",
    "href": "posts/alt-fuel-station-analysis/index.html#conclusion",
    "title": "Alternative Fuel Stations",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis focused on understanding the distribution electric fueling stations across the US between the years 2010-2020 and attempted to better understand the relationship between a state’s population and the number of stations in the state. Electric vehicle charging station growth rate seems to coincide with the release of popular electric cars which drive larger adoption of electric vehicles and increase the demand for electric charging stations. We also showed a strong correlation between the population density of a state and the number of electric charging stations per square mile in that state. From this, we were able to infer that for every 1% increase in the population density of a state, we can expect, on average, a 1.04% \\(\\pm\\) 0.12% increase in the station density for that state. Looking at an animation of this trend through time, the slope and strength of correlation appear to be consistent through much of 2010 - 2020."
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Link to Github repository"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#setup",
    "href": "posts/palmer-penguins/index.html#setup",
    "title": "Palmer Penguins",
    "section": "Setup",
    "text": "Setup\nLoad the ncessary libraries and import the dataset\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(scales)\ntheme_set(theme_light())\n\n#tt <- tidytuesdayR::tt_load('2020-07-28')\n#penguins_raw <- tt$penguins\npenguins_raw <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#clean-data",
    "href": "posts/palmer-penguins/index.html#clean-data",
    "title": "Palmer Penguins",
    "section": "Clean Data",
    "text": "Clean Data\nThe penguin data has a few missing observations for bill length and depth, flipper length, and body mass. Based on a consistency found amongst species in the data, the missing values are replaced by the mean for that species.\n\npenguins <- penguins_raw |> \n  # convert to factors\n  mutate(species = factor(species), island = factor(island), sex = factor(sex)) |> \n  group_by(species) |> \n  # replace missing values with average for species\n  mutate(\n    bill_length_mm = ifelse(is.na(bill_length_mm), mean(bill_length_mm, na.rm=TRUE), bill_length_mm),\n    bill_depth_mm = ifelse(is.na(bill_depth_mm), mean(bill_depth_mm, na.rm=TRUE), bill_depth_mm),\n    flipper_length_mm = ifelse(is.na(flipper_length_mm), mean(flipper_length_mm, na.rm=TRUE), flipper_length_mm), \n    body_mass_g = ifelse(is.na(body_mass_g), mean(body_mass_g, na.rm=TRUE), body_mass_g)\n    ) |> \n  ungroup()"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#split-data",
    "href": "posts/palmer-penguins/index.html#split-data",
    "title": "Palmer Penguins",
    "section": "Split Data",
    "text": "Split Data\nThe penguin data is split into training data (70%) to train the K-Nearest Neighbors classifier and test data (30%) to test the accuracy of the classifier model at using the beak length and depth for predicting the species.\n\nset.seed(42)\n# split the data into 70% training and 30% test\npenguin_split<- initial_split(penguins, prop=.7)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#build-model",
    "href": "posts/palmer-penguins/index.html#build-model",
    "title": "Palmer Penguins",
    "section": "Build Model",
    "text": "Build Model\nMeasurements of penguins’ bill length and depth show distinct clusters grouped by the penguin’s species. Because of the separation in the species clusters, we can build a classifier to predict which species a particular penguin belongs to given its bill depth and length measurements.\n\npenguin_train |> \n  ggplot(aes(bill_length_mm, bill_depth_mm, color=species)) +\n  geom_point() + \n  labs(title = \"Penguin Species Predictors\", x=\"Bill Length (mm)\", y=\"Bill Depth (mm)\", color=\"Species\")\n\n\n\n\nWe will use a K-nearest neighbors classifier to predict the penguin species. We start by initializing the nearest neighbor model and instantiate it to use six neighbors. This instructs the model to construct a decision boundary based on the six closest training observations. This means for any test observation, the model will find the six closest training observations, find the proportion of each species within those closest six, then assign the species with the highest proportion to the test observation.\nLower nearest neighbor values have lower bias and higher variance, which can lead to over-fitting of the data. The over-fitting is caused by the model using fewer points to classify the test observation, resulting in a more flexible boundary line that more closely takes the shape of the data it is trained on. Higher nearest neighbor values lead to a less flexible boundary (closer to linear line) with higher bias and lower variance since they consider more points to classify the test observation. This can lead to a decision boundary that does not fit the data as as closely as more flexible boundaries, but has more consistent accuracy between test samples. A nearest neighbor value of six was chosen based on model accuracy results against the test data.\n\n# create and fit the nearest neighbor classifier\nknn_mod <- nearest_neighbor(\n  mode = \"classification\",\n  neighbors = 6) |> \n  fit(formula = species ~ bill_length_mm + bill_depth_mm, data = penguin_train)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#make-predictions",
    "href": "posts/palmer-penguins/index.html#make-predictions",
    "title": "Palmer Penguins",
    "section": "Make Predictions",
    "text": "Make Predictions\nUsing the k-nearest neighbors model and the test data, we can predict the penguin species for the bill length and depth observations in the test data.\n\n# predict classification for test data and bind predictions to dataframe\nknn_pred <- penguin_test %>%\n  bind_cols(predict(knn_mod, .)) |> \n  mutate(species_pred = .pred_class)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#evaluate-model",
    "href": "posts/palmer-penguins/index.html#evaluate-model",
    "title": "Palmer Penguins",
    "section": "Evaluate Model",
    "text": "Evaluate Model\nWith the prediction results, we can construct a confusion matrix showing where the model correctly and incorrectly predicted the penguin species.\n\nconf_mat <- knn_pred |> \n  # group by the actual/predicted combinations\n  group_by(species, species_pred) |> \n  # count the number of occurences for each combination\n  summarize(count = n(), .groups=\"drop\") |> \n  # extend the data to ensure each combination is in the data\n  complete(species, species_pred) |> \n  # replace the nas from the complete() function with 0s\n  mutate(count = replace_na(count, 0))\n\n  # plot the confusion matrix\n  ggplot(conf_mat, aes(species, fct_rev(species_pred), fill=count)) + \n  geom_tile(show.legend = FALSE) + \n  geom_text(aes(label = count)) +\n  scale_fill_gradient2(low=\"white\", high=\"orange\") + \n  labs(\n    title = \"Penguin Species Confusion Matrix\", \n    x=\"Actual Species\", \n    y=\"Predicted Species\") +\n  theme(\n    panel.border = element_blank(),\n    panel.grid = element_blank(),\n    axis.ticks = element_blank()\n  )\n\n\n\n\nA closer look at the metrics show that the classifier was effective at predicting the penguin species, with over 95% for accuracy, precision, recall, and specificity.\n\nmetrics <- accuracy(knn_pred, species, species_pred) |> \n  rbind(precision(knn_pred, species, species_pred)) |> \n  rbind(recall(knn_pred, species, species_pred)) |> \n  rbind(specificity(knn_pred, species, species_pred)) |> \n  transmute(Metric = .metric, Estimate = percent(.estimate, accuracy = 0.1))\n\nknitr::kable(metrics, caption = \"Species prediction metrics\")\n\n\nSpecies prediction metrics\n\n\nMetric\nEstimate\n\n\n\n\naccuracy\n97.1%\n\n\nprecision\n96.6%\n\n\nrecall\n95.6%\n\n\nspecificity\n98.5%"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects",
    "section": "",
    "text": "R\n\n\ntidy-tuesday\n\n\nanalysis\n\n\ndata-viz\n\n\neda\n\n\nanimation\n\n\nregression\n\n\ninference\n\n\n\nTidy Tuesday R analysis on alternative fuel station locations with data visualization and linear regression model inference.\n\n\n\nSean Hoyt\n\n\nMay 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nudacity\n\n\nanalysis\n\n\nclassification\n\n\nmodel-eval\n\n\n\nUdacity supervised learning project: construct and evaluate multiple models to predict whether an individual makes more than $50,000.\n\n\n\nSean Hoyt\n\n\nMay 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\ntidy-tuesday\n\n\nanalysis\n\n\nclassification\n\n\n\nPredicting penguin species using the bill length and depth.\n\n\n\nSean Hoyt\n\n\nApr 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nudacity\n\n\nanalysis\n\n\ndata-viz\n\n\n\nUdacity data visualization project\n\n\n\nSean Hoyt\n\n\nApr 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\neda\n\n\nudacity\n\n\nanalysis\n\n\ndata-viz\n\n\n\nUdacity data visualization project - EDA\n\n\n\nSean Hoyt\n\n\nApr 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nudacity\n\n\nanalysis\n\n\ndata-cleaning\n\n\n\nUdacity data wrangling project\n\n\n\nSean Hoyt\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nudacity\n\n\nanalysis\n\n\nregression\n\n\ninference\n\n\n\nUdacity A/B test project\n\n\n\nSean Hoyt\n\n\nMar 15, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]